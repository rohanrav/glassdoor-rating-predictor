{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80b357e0-bbbc-4b40-93d2-2f775e36fa0e",
   "metadata": {},
   "source": [
    "# Q1\n",
    "\n",
    "### Deep Learning (Neural Network with Vector Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13e1b95a-d922-4d7c-9efc-0d0582d74a8d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (2.0.1)\n",
      "Requirement already satisfied: transformers in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (4.47.0)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.5.3)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.5.2)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (4.66.5)\n",
      "Requirement already satisfied: textblob in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (0.18.0.post0)\n",
      "Requirement already satisfied: sentence_transformers in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (3.3.1)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.0.1) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.0.1) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.0.1) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.0.1) (3.4)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.0.1) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.0.1) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.0.1) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.0.1) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.0.1) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.0.1) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.0.1) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.0.1) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.0.1) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.0.1) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.0.1) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.0.1) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.0.1) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (75.1.0)\n",
      "Requirement already satisfied: wheel in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.44.0)\n",
      "Requirement already satisfied: cmake in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from triton==2.0.0->torch==2.0.1) (3.31.1)\n",
      "Requirement already satisfied: lit in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from triton==2.0.0->torch==2.0.1) (18.1.8)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (0.26.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: nltk>=3.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: Pillow in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sentence_transformers) (11.0.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk>=3.8->textblob) (8.1.7)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jinja2->torch==2.0.1) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sympy->torch==2.0.1) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch==2.0.1 transformers pandas numpy scikit-learn tqdm textblob sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58058069-a6bc-4d8f-b4c2-247f88990c18",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (4.47.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (0.26.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b78f0525-f361-44be-a99f-dc5e609bb351",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from textblob import TextBlob\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a7e673d-6aa2-44f5-8479-2e2b084ead61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class BertEmbedder:\n",
    "    def __init__(self, model_name='distilbert-base-uncased'):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        self.model.to(self.device)\n",
    "        self.batch_size = 32\n",
    "        self.max_length = 256  # Reduced from 512 for efficiency\n",
    "\n",
    "    def process_batch(self, texts):\n",
    "        # Convert each text to string and strip whitespace\n",
    "        texts = [str(text).strip() for text in texts]\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Move inputs to device\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Get embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            # Use mean pooling instead of just [CLS] token\n",
    "            attention_mask = inputs['attention_mask']\n",
    "            token_embeddings = outputs.last_hidden_state\n",
    "            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "            embeddings = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "            return embeddings.cpu().numpy()\n",
    "\n",
    "    def get_embeddings(self, df):\n",
    "        # Process pros, cons, and headlines separately\n",
    "        print(\"Processing pros embeddings...\")\n",
    "        pros_emb = self._process_text_column(df['pros'])\n",
    "        print(\"Processing cons embeddings...\")\n",
    "        cons_emb = self._process_text_column(df['cons'])\n",
    "        print(\"Processing headline embeddings...\")\n",
    "        headline_emb = self._process_text_column(df['headline'])\n",
    "        \n",
    "        return pros_emb, cons_emb, headline_emb\n",
    "    \n",
    "    def _process_text_column(self, texts):\n",
    "        embeddings = []\n",
    "        for i in tqdm(range(0, len(texts), self.batch_size)):\n",
    "            batch = texts[i:i + self.batch_size].tolist()\n",
    "            batch_embeddings = self.process_batch(batch)\n",
    "            embeddings.append(batch_embeddings)\n",
    "        return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80e22945-737f-4737-8509-cdd86fc74a6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.firm_stats = {}\n",
    "        self.bert = BertEmbedder()\n",
    "        self.min_reviews_for_firm = 50  # Increased threshold for firm features\n",
    "        \n",
    "    def _calculate_firm_statistics(self, df):\n",
    "        \"\"\"Calculate enhanced firm statistics.\"\"\"\n",
    "        firm_stats = df.groupby('firm').agg({\n",
    "            'rating': ['count', 'mean', 'std', 'median']\n",
    "        }).round(3)\n",
    "        firm_stats.columns = ['count', 'mean', 'std', 'median']\n",
    "        firm_stats = firm_stats.reset_index()\n",
    "        \n",
    "        # Identify firms with enough reviews\n",
    "        frequent_firms = firm_stats[firm_stats['count'] >= self.min_reviews_for_firm]['firm'].tolist()\n",
    "        \n",
    "        self.firm_stats = {\n",
    "            'frequent_firms': set(frequent_firms),\n",
    "            'stats': firm_stats.set_index('firm').to_dict(),\n",
    "            'global_mean': df['rating'].mean(),\n",
    "            'global_std': df['rating'].std(),\n",
    "            'global_median': df['rating'].median()\n",
    "        }\n",
    "        \n",
    "    def _get_sentiment_features(self, text):\n",
    "        \"\"\"Extract sentiment features from text using TextBlob.\"\"\"\n",
    "        if not isinstance(text, str) or text.strip() == '':\n",
    "            return 0.0\n",
    "        \n",
    "        try:\n",
    "            blob = TextBlob(text.strip())\n",
    "            return blob.sentiment.polarity\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def _scale_features(self, df, is_test=False):\n",
    "        if not is_test:\n",
    "            df[self.numerical_features] = self.scaler.fit_transform(df[self.numerical_features])\n",
    "        else:\n",
    "            df[self.numerical_features] = self.scaler.transform(df[self.numerical_features])\n",
    "        return df\n",
    "    \n",
    "    def _create_features(self, df):\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Handle missing values\n",
    "        df['pros'] = df['pros'].fillna('').astype(str)\n",
    "        df['cons'] = df['cons'].fillna('').astype(str)\n",
    "        df['headline'] = df['headline'].fillna('').astype(str)\n",
    "        \n",
    "        # Basic text features\n",
    "        df['pros_length'] = df['pros'].str.strip().str.len()\n",
    "        df['cons_length'] = df['cons'].str.strip().str.len()\n",
    "        df['pros_cons_ratio'] = (df['pros_length'] + 1) / (df['cons_length'] + 1)\n",
    "        \n",
    "        # Sentiment\n",
    "        print('Sentiment...')\n",
    "        df['cons_sentiment'] = df['cons'].apply(self._get_sentiment_features)\n",
    "        df['pros_sentiment'] = df['pros'].apply(self._get_sentiment_features)\n",
    "        df['headline_sentiment'] = df['headline'].apply(self._get_sentiment_features)\n",
    "        \n",
    "        # Enhanced firm features\n",
    "        df['is_frequent_firm'] = df['firm'].isin(self.firm_stats['frequent_firms']).astype(int)\n",
    "        df['firm_mean_rating'] = df['firm'].map(self.firm_stats['stats']['mean']).fillna(self.firm_stats['global_mean'])\n",
    "        df['firm_rating_std'] = df['firm'].map(self.firm_stats['stats']['std']).fillna(self.firm_stats['global_std'])\n",
    "        \n",
    "        # Year features (bucketed)\n",
    "        df['year_bucket'] = pd.qcut(df['year_review'], q=5, labels=['very_old', 'old', 'medium', 'recent', 'very_recent'])\n",
    "        year_dummies = pd.get_dummies(df['year_bucket'], prefix='year')\n",
    "        \n",
    "        # Get BERT embeddings\n",
    "        print(\"Getting BERT embeddings...\")\n",
    "        pros_emb, cons_emb, headline_emb = self.bert.get_embeddings(df)\n",
    "        \n",
    "        # Create feature matrix\n",
    "        emb_features = np.hstack([pros_emb, cons_emb, headline_emb])\n",
    "        emb_cols = [f'emb_{i}' for i in range(emb_features.shape[1])]\n",
    "        \n",
    "        # Combine all features\n",
    "        numerical_features = ['pros_length', 'cons_length', 'pros_cons_ratio',\n",
    "                            'firm_mean_rating', 'firm_rating_std', 'cons_sentiment', 'pros_sentiment' ,'headline_sentiment']\n",
    "        \n",
    "        df_final = pd.concat([\n",
    "            df[numerical_features],\n",
    "            year_dummies,\n",
    "            pd.DataFrame(emb_features, columns=emb_cols)\n",
    "        ], axis=1)\n",
    "        \n",
    "        return df_final\n",
    "    \n",
    "    def preprocess_data(self, df_small, df_large, df_test, random_state=42):\n",
    "        print(\"Combining training datasets...\")\n",
    "        df_train = pd.concat([df_small, df_large], axis=0, ignore_index=True)\n",
    "        \n",
    "        print(\"Calculating firm statistics...\")\n",
    "        self._calculate_firm_statistics(df_train)\n",
    "        \n",
    "        print(\"Processing training data...\")\n",
    "        X_train_full = self._create_features(df_train)\n",
    "        \n",
    "        print(\"Processing test data...\")\n",
    "        X_test = self._create_features(df_test)\n",
    "        \n",
    "        y_train_full = df_train['rating']\n",
    "        \n",
    "        # Create stratified split with larger validation set\n",
    "        print(\"Creating validation split...\")\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train_full, y_train_full,\n",
    "            test_size=0.15,  # Increased validation size\n",
    "            random_state=random_state,\n",
    "            stratify=y_train_full\n",
    "        )\n",
    "        \n",
    "        # Scale features\n",
    "        numerical_features = X_train.select_dtypes(include=['float64']).columns\n",
    "        scaler = StandardScaler()\n",
    "        X_train[numerical_features] = scaler.fit_transform(X_train[numerical_features])\n",
    "        X_val[numerical_features] = scaler.transform(X_val[numerical_features])\n",
    "        X_test[numerical_features] = scaler.transform(X_test[numerical_features])\n",
    "        \n",
    "        return X_train, X_val, y_train, y_val, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4fad7b26-4cad-4c43-800c-1c0d4ccd8fdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(small_train_path, large_train_path, test_path):\n",
    "    \"\"\"Load and preprocess all datasets.\"\"\"\n",
    "    print(\"Loading datasets...\")\n",
    "    df_small = pd.read_csv(small_train_path)\n",
    "    df_large = pd.read_csv(large_train_path)\n",
    "    df_test = pd.read_csv(test_path)\n",
    "    \n",
    "    preprocessor = DataPreprocessor()\n",
    "    return preprocessor.preprocess_data(df_small, df_large, df_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6cb7ff-1597-4b15-9b50-ca48dd884354",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Using device: cuda\n",
      "Combining training datasets...\n",
      "Calculating firm statistics...\n",
      "Processing training data...\n",
      "Sentiment...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18750/18750 [13:31<00:00, 23.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing headline embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18750/18750 [02:56<00:00, 106.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test data...\n",
      "Sentiment...\n",
      "Getting BERT embeddings...\n",
      "Processing pros embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125 [01:56<00:00, 26.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing cons embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125 [03:21<00:00, 15.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing headline embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125 [00:30<00:00, 102.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating validation split...\n",
      "\n",
      "Processed data shapes:\n",
      "X_train: (510000, 2317)\n",
      "X_val: (90000, 2317)\n",
      "y_train: (510000,)\n",
      "y_val: (90000,)\n",
      "X_test: (100000, 2317)\n",
      "\n",
      "Features created:\n",
      "- pros_length\n",
      "- cons_length\n",
      "- pros_cons_ratio\n",
      "- firm_mean_rating\n",
      "- firm_rating_std\n",
      "- cons_sentiment\n",
      "- pros_sentiment\n",
      "- headline_sentiment\n",
      "- year_very_old\n",
      "- year_old\n",
      "- year_medium\n",
      "- year_recent\n",
      "- year_very_recent\n",
      "- emb_0\n",
      "- emb_1\n",
      "- emb_2\n",
      "- emb_3\n",
      "- emb_4\n",
      "- emb_5\n",
      "- emb_6\n",
      "- emb_7\n",
      "- emb_8\n",
      "- emb_9\n",
      "- emb_10\n",
      "- emb_11\n",
      "- emb_12\n",
      "- emb_13\n",
      "- emb_14\n",
      "- emb_15\n",
      "- emb_16\n",
      "- emb_17\n",
      "- emb_18\n",
      "- emb_19\n",
      "- emb_20\n",
      "- emb_21\n",
      "- emb_22\n",
      "- emb_23\n",
      "- emb_24\n",
      "- emb_25\n",
      "- emb_26\n",
      "- emb_27\n",
      "- emb_28\n",
      "- emb_29\n",
      "- emb_30\n",
      "- emb_31\n",
      "- emb_32\n",
      "- emb_33\n",
      "- emb_34\n",
      "- emb_35\n",
      "- emb_36\n",
      "- emb_37\n",
      "- emb_38\n",
      "- emb_39\n",
      "- emb_40\n",
      "- emb_41\n",
      "- emb_42\n",
      "- emb_43\n",
      "- emb_44\n",
      "- emb_45\n",
      "- emb_46\n",
      "- emb_47\n",
      "- emb_48\n",
      "- emb_49\n",
      "- emb_50\n",
      "- emb_51\n",
      "- emb_52\n",
      "- emb_53\n",
      "- emb_54\n",
      "- emb_55\n",
      "- emb_56\n",
      "- emb_57\n",
      "- emb_58\n",
      "- emb_59\n",
      "- emb_60\n",
      "- emb_61\n",
      "- emb_62\n",
      "- emb_63\n",
      "- emb_64\n",
      "- emb_65\n",
      "- emb_66\n",
      "- emb_67\n",
      "- emb_68\n",
      "- emb_69\n",
      "- emb_70\n",
      "- emb_71\n",
      "- emb_72\n",
      "- emb_73\n",
      "- emb_74\n",
      "- emb_75\n",
      "- emb_76\n",
      "- emb_77\n",
      "- emb_78\n",
      "- emb_79\n",
      "- emb_80\n",
      "- emb_81\n",
      "- emb_82\n",
      "- emb_83\n",
      "- emb_84\n",
      "- emb_85\n",
      "- emb_86\n",
      "- emb_87\n",
      "- emb_88\n",
      "- emb_89\n",
      "- emb_90\n",
      "- emb_91\n",
      "- emb_92\n",
      "- emb_93\n",
      "- emb_94\n",
      "- emb_95\n",
      "- emb_96\n",
      "- emb_97\n",
      "- emb_98\n",
      "- emb_99\n",
      "- emb_100\n",
      "- emb_101\n",
      "- emb_102\n",
      "- emb_103\n",
      "- emb_104\n",
      "- emb_105\n",
      "- emb_106\n",
      "- emb_107\n",
      "- emb_108\n",
      "- emb_109\n",
      "- emb_110\n",
      "- emb_111\n",
      "- emb_112\n",
      "- emb_113\n",
      "- emb_114\n",
      "- emb_115\n",
      "- emb_116\n",
      "- emb_117\n",
      "- emb_118\n",
      "- emb_119\n",
      "- emb_120\n",
      "- emb_121\n",
      "- emb_122\n",
      "- emb_123\n",
      "- emb_124\n",
      "- emb_125\n",
      "- emb_126\n",
      "- emb_127\n",
      "- emb_128\n",
      "- emb_129\n",
      "- emb_130\n",
      "- emb_131\n",
      "- emb_132\n",
      "- emb_133\n",
      "- emb_134\n",
      "- emb_135\n",
      "- emb_136\n",
      "- emb_137\n",
      "- emb_138\n",
      "- emb_139\n",
      "- emb_140\n",
      "- emb_141\n",
      "- emb_142\n",
      "- emb_143\n",
      "- emb_144\n",
      "- emb_145\n",
      "- emb_146\n",
      "- emb_147\n",
      "- emb_148\n",
      "- emb_149\n",
      "- emb_150\n",
      "- emb_151\n",
      "- emb_152\n",
      "- emb_153\n",
      "- emb_154\n",
      "- emb_155\n",
      "- emb_156\n",
      "- emb_157\n",
      "- emb_158\n",
      "- emb_159\n",
      "- emb_160\n",
      "- emb_161\n",
      "- emb_162\n",
      "- emb_163\n",
      "- emb_164\n",
      "- emb_165\n",
      "- emb_166\n",
      "- emb_167\n",
      "- emb_168\n",
      "- emb_169\n",
      "- emb_170\n",
      "- emb_171\n",
      "- emb_172\n",
      "- emb_173\n",
      "- emb_174\n",
      "- emb_175\n",
      "- emb_176\n",
      "- emb_177\n",
      "- emb_178\n",
      "- emb_179\n",
      "- emb_180\n",
      "- emb_181\n",
      "- emb_182\n",
      "- emb_183\n",
      "- emb_184\n",
      "- emb_185\n",
      "- emb_186\n",
      "- emb_187\n",
      "- emb_188\n",
      "- emb_189\n",
      "- emb_190\n",
      "- emb_191\n",
      "- emb_192\n",
      "- emb_193\n",
      "- emb_194\n",
      "- emb_195\n",
      "- emb_196\n",
      "- emb_197\n",
      "- emb_198\n",
      "- emb_199\n",
      "- emb_200\n",
      "- emb_201\n",
      "- emb_202\n",
      "- emb_203\n",
      "- emb_204\n",
      "- emb_205\n",
      "- emb_206\n",
      "- emb_207\n",
      "- emb_208\n",
      "- emb_209\n",
      "- emb_210\n",
      "- emb_211\n",
      "- emb_212\n",
      "- emb_213\n",
      "- emb_214\n",
      "- emb_215\n",
      "- emb_216\n",
      "- emb_217\n",
      "- emb_218\n",
      "- emb_219\n",
      "- emb_220\n",
      "- emb_221\n",
      "- emb_222\n",
      "- emb_223\n",
      "- emb_224\n",
      "- emb_225\n",
      "- emb_226\n",
      "- emb_227\n",
      "- emb_228\n",
      "- emb_229\n",
      "- emb_230\n",
      "- emb_231\n",
      "- emb_232\n",
      "- emb_233\n",
      "- emb_234\n",
      "- emb_235\n",
      "- emb_236\n",
      "- emb_237\n",
      "- emb_238\n",
      "- emb_239\n",
      "- emb_240\n",
      "- emb_241\n",
      "- emb_242\n",
      "- emb_243\n",
      "- emb_244\n",
      "- emb_245\n",
      "- emb_246\n",
      "- emb_247\n",
      "- emb_248\n",
      "- emb_249\n",
      "- emb_250\n",
      "- emb_251\n",
      "- emb_252\n",
      "- emb_253\n",
      "- emb_254\n",
      "- emb_255\n",
      "- emb_256\n",
      "- emb_257\n",
      "- emb_258\n",
      "- emb_259\n",
      "- emb_260\n",
      "- emb_261\n",
      "- emb_262\n",
      "- emb_263\n",
      "- emb_264\n",
      "- emb_265\n",
      "- emb_266\n",
      "- emb_267\n",
      "- emb_268\n",
      "- emb_269\n",
      "- emb_270\n",
      "- emb_271\n",
      "- emb_272\n",
      "- emb_273\n",
      "- emb_274\n",
      "- emb_275\n",
      "- emb_276\n",
      "- emb_277\n",
      "- emb_278\n",
      "- emb_279\n",
      "- emb_280\n",
      "- emb_281\n",
      "- emb_282\n",
      "- emb_283\n",
      "- emb_284\n",
      "- emb_285\n",
      "- emb_286\n",
      "- emb_287\n",
      "- emb_288\n",
      "- emb_289\n",
      "- emb_290\n",
      "- emb_291\n",
      "- emb_292\n",
      "- emb_293\n",
      "- emb_294\n",
      "- emb_295\n",
      "- emb_296\n",
      "- emb_297\n",
      "- emb_298\n",
      "- emb_299\n",
      "- emb_300\n",
      "- emb_301\n",
      "- emb_302\n",
      "- emb_303\n",
      "- emb_304\n",
      "- emb_305\n",
      "- emb_306\n",
      "- emb_307\n",
      "- emb_308\n",
      "- emb_309\n",
      "- emb_310\n",
      "- emb_311\n",
      "- emb_312\n",
      "- emb_313\n",
      "- emb_314\n",
      "- emb_315\n",
      "- emb_316\n",
      "- emb_317\n",
      "- emb_318\n",
      "- emb_319\n",
      "- emb_320\n",
      "- emb_321\n",
      "- emb_322\n",
      "- emb_323\n",
      "- emb_324\n",
      "- emb_325\n",
      "- emb_326\n",
      "- emb_327\n",
      "- emb_328\n",
      "- emb_329\n",
      "- emb_330\n",
      "- emb_331\n",
      "- emb_332\n",
      "- emb_333\n",
      "- emb_334\n",
      "- emb_335\n",
      "- emb_336\n",
      "- emb_337\n",
      "- emb_338\n",
      "- emb_339\n",
      "- emb_340\n",
      "- emb_341\n",
      "- emb_342\n",
      "- emb_343\n",
      "- emb_344\n",
      "- emb_345\n",
      "- emb_346\n",
      "- emb_347\n",
      "- emb_348\n",
      "- emb_349\n",
      "- emb_350\n",
      "- emb_351\n",
      "- emb_352\n",
      "- emb_353\n",
      "- emb_354\n",
      "- emb_355\n",
      "- emb_356\n",
      "- emb_357\n",
      "- emb_358\n",
      "- emb_359\n",
      "- emb_360\n",
      "- emb_361\n",
      "- emb_362\n",
      "- emb_363\n",
      "- emb_364\n",
      "- emb_365\n",
      "- emb_366\n",
      "- emb_367\n",
      "- emb_368\n",
      "- emb_369\n",
      "- emb_370\n",
      "- emb_371\n",
      "- emb_372\n",
      "- emb_373\n",
      "- emb_374\n",
      "- emb_375\n",
      "- emb_376\n",
      "- emb_377\n",
      "- emb_378\n",
      "- emb_379\n",
      "- emb_380\n",
      "- emb_381\n",
      "- emb_382\n",
      "- emb_383\n",
      "- emb_384\n",
      "- emb_385\n",
      "- emb_386\n",
      "- emb_387\n",
      "- emb_388\n",
      "- emb_389\n",
      "- emb_390\n",
      "- emb_391\n",
      "- emb_392\n",
      "- emb_393\n",
      "- emb_394\n",
      "- emb_395\n",
      "- emb_396\n",
      "- emb_397\n",
      "- emb_398\n",
      "- emb_399\n",
      "- emb_400\n",
      "- emb_401\n",
      "- emb_402\n",
      "- emb_403\n",
      "- emb_404\n",
      "- emb_405\n",
      "- emb_406\n",
      "- emb_407\n",
      "- emb_408\n",
      "- emb_409\n",
      "- emb_410\n",
      "- emb_411\n",
      "- emb_412\n",
      "- emb_413\n",
      "- emb_414\n",
      "- emb_415\n",
      "- emb_416\n",
      "- emb_417\n",
      "- emb_418\n",
      "- emb_419\n",
      "- emb_420\n",
      "- emb_421\n",
      "- emb_422\n",
      "- emb_423\n",
      "- emb_424\n",
      "- emb_425\n",
      "- emb_426\n",
      "- emb_427\n",
      "- emb_428\n",
      "- emb_429\n",
      "- emb_430\n",
      "- emb_431\n",
      "- emb_432\n",
      "- emb_433\n",
      "- emb_434\n",
      "- emb_435\n",
      "- emb_436\n",
      "- emb_437\n",
      "- emb_438\n",
      "- emb_439\n",
      "- emb_440\n",
      "- emb_441\n",
      "- emb_442\n",
      "- emb_443\n",
      "- emb_444\n",
      "- emb_445\n",
      "- emb_446\n",
      "- emb_447\n",
      "- emb_448\n",
      "- emb_449\n",
      "- emb_450\n",
      "- emb_451\n",
      "- emb_452\n",
      "- emb_453\n",
      "- emb_454\n",
      "- emb_455\n",
      "- emb_456\n",
      "- emb_457\n",
      "- emb_458\n",
      "- emb_459\n",
      "- emb_460\n",
      "- emb_461\n",
      "- emb_462\n",
      "- emb_463\n",
      "- emb_464\n",
      "- emb_465\n",
      "- emb_466\n",
      "- emb_467\n",
      "- emb_468\n",
      "- emb_469\n",
      "- emb_470\n",
      "- emb_471\n",
      "- emb_472\n",
      "- emb_473\n",
      "- emb_474\n",
      "- emb_475\n",
      "- emb_476\n",
      "- emb_477\n",
      "- emb_478\n",
      "- emb_479\n",
      "- emb_480\n",
      "- emb_481\n",
      "- emb_482\n",
      "- emb_483\n",
      "- emb_484\n",
      "- emb_485\n",
      "- emb_486\n",
      "- emb_487\n",
      "- emb_488\n",
      "- emb_489\n",
      "- emb_490\n",
      "- emb_491\n",
      "- emb_492\n",
      "- emb_493\n",
      "- emb_494\n",
      "- emb_495\n",
      "- emb_496\n",
      "- emb_497\n",
      "- emb_498\n",
      "- emb_499\n",
      "- emb_500\n",
      "- emb_501\n",
      "- emb_502\n",
      "- emb_503\n",
      "- emb_504\n",
      "- emb_505\n",
      "- emb_506\n",
      "- emb_507\n",
      "- emb_508\n",
      "- emb_509\n",
      "- emb_510\n",
      "- emb_511\n",
      "- emb_512\n",
      "- emb_513\n",
      "- emb_514\n",
      "- emb_515\n",
      "- emb_516\n",
      "- emb_517\n",
      "- emb_518\n",
      "- emb_519\n",
      "- emb_520\n",
      "- emb_521\n",
      "- emb_522\n",
      "- emb_523\n",
      "- emb_524\n",
      "- emb_525\n",
      "- emb_526\n",
      "- emb_527\n",
      "- emb_528\n",
      "- emb_529\n",
      "- emb_530\n",
      "- emb_531\n",
      "- emb_532\n",
      "- emb_533\n",
      "- emb_534\n",
      "- emb_535\n",
      "- emb_536\n",
      "- emb_537\n",
      "- emb_538\n",
      "- emb_539\n",
      "- emb_540\n",
      "- emb_541\n",
      "- emb_542\n",
      "- emb_543\n",
      "- emb_544\n",
      "- emb_545\n",
      "- emb_546\n",
      "- emb_547\n",
      "- emb_548\n",
      "- emb_549\n",
      "- emb_550\n",
      "- emb_551\n",
      "- emb_552\n",
      "- emb_553\n",
      "- emb_554\n",
      "- emb_555\n",
      "- emb_556\n",
      "- emb_557\n",
      "- emb_558\n",
      "- emb_559\n",
      "- emb_560\n",
      "- emb_561\n",
      "- emb_562\n",
      "- emb_563\n",
      "- emb_564\n",
      "- emb_565\n",
      "- emb_566\n",
      "- emb_567\n",
      "- emb_568\n",
      "- emb_569\n",
      "- emb_570\n",
      "- emb_571\n",
      "- emb_572\n",
      "- emb_573\n",
      "- emb_574\n",
      "- emb_575\n",
      "- emb_576\n",
      "- emb_577\n",
      "- emb_578\n",
      "- emb_579\n",
      "- emb_580\n",
      "- emb_581\n",
      "- emb_582\n",
      "- emb_583\n",
      "- emb_584\n",
      "- emb_585\n",
      "- emb_586\n",
      "- emb_587\n",
      "- emb_588\n",
      "- emb_589\n",
      "- emb_590\n",
      "- emb_591\n",
      "- emb_592\n",
      "- emb_593\n",
      "- emb_594\n",
      "- emb_595\n",
      "- emb_596\n",
      "- emb_597\n",
      "- emb_598\n",
      "- emb_599\n",
      "- emb_600\n",
      "- emb_601\n",
      "- emb_602\n",
      "- emb_603\n",
      "- emb_604\n",
      "- emb_605\n",
      "- emb_606\n",
      "- emb_607\n",
      "- emb_608\n",
      "- emb_609\n",
      "- emb_610\n",
      "- emb_611\n",
      "- emb_612\n",
      "- emb_613\n",
      "- emb_614\n",
      "- emb_615\n",
      "- emb_616\n",
      "- emb_617\n",
      "- emb_618\n",
      "- emb_619\n",
      "- emb_620\n",
      "- emb_621\n",
      "- emb_622\n",
      "- emb_623\n",
      "- emb_624\n",
      "- emb_625\n",
      "- emb_626\n",
      "- emb_627\n",
      "- emb_628\n",
      "- emb_629\n",
      "- emb_630\n",
      "- emb_631\n",
      "- emb_632\n",
      "- emb_633\n",
      "- emb_634\n",
      "- emb_635\n",
      "- emb_636\n",
      "- emb_637\n",
      "- emb_638\n",
      "- emb_639\n",
      "- emb_640\n",
      "- emb_641\n",
      "- emb_642\n",
      "- emb_643\n",
      "- emb_644\n",
      "- emb_645\n",
      "- emb_646\n",
      "- emb_647\n",
      "- emb_648\n",
      "- emb_649\n",
      "- emb_650\n",
      "- emb_651\n",
      "- emb_652\n",
      "- emb_653\n",
      "- emb_654\n",
      "- emb_655\n",
      "- emb_656\n",
      "- emb_657\n",
      "- emb_658\n",
      "- emb_659\n",
      "- emb_660\n",
      "- emb_661\n",
      "- emb_662\n",
      "- emb_663\n",
      "- emb_664\n",
      "- emb_665\n",
      "- emb_666\n",
      "- emb_667\n",
      "- emb_668\n",
      "- emb_669\n",
      "- emb_670\n",
      "- emb_671\n",
      "- emb_672\n",
      "- emb_673\n",
      "- emb_674\n",
      "- emb_675\n",
      "- emb_676\n",
      "- emb_677\n",
      "- emb_678\n",
      "- emb_679\n",
      "- emb_680\n",
      "- emb_681\n",
      "- emb_682\n",
      "- emb_683\n",
      "- emb_684\n",
      "- emb_685\n",
      "- emb_686\n",
      "- emb_687\n",
      "- emb_688\n",
      "- emb_689\n",
      "- emb_690\n",
      "- emb_691\n",
      "- emb_692\n",
      "- emb_693\n",
      "- emb_694\n",
      "- emb_695\n",
      "- emb_696\n",
      "- emb_697\n",
      "- emb_698\n",
      "- emb_699\n",
      "- emb_700\n",
      "- emb_701\n",
      "- emb_702\n",
      "- emb_703\n",
      "- emb_704\n",
      "- emb_705\n",
      "- emb_706\n",
      "- emb_707\n",
      "- emb_708\n",
      "- emb_709\n",
      "- emb_710\n",
      "- emb_711\n",
      "- emb_712\n",
      "- emb_713\n",
      "- emb_714\n",
      "- emb_715\n",
      "- emb_716\n",
      "- emb_717\n",
      "- emb_718\n",
      "- emb_719\n",
      "- emb_720\n",
      "- emb_721\n",
      "- emb_722\n",
      "- emb_723\n",
      "- emb_724\n",
      "- emb_725\n",
      "- emb_726\n",
      "- emb_727\n",
      "- emb_728\n",
      "- emb_729\n",
      "- emb_730\n",
      "- emb_731\n",
      "- emb_732\n",
      "- emb_733\n",
      "- emb_734\n",
      "- emb_735\n",
      "- emb_736\n",
      "- emb_737\n",
      "- emb_738\n",
      "- emb_739\n",
      "- emb_740\n",
      "- emb_741\n",
      "- emb_742\n",
      "- emb_743\n",
      "- emb_744\n",
      "- emb_745\n",
      "- emb_746\n",
      "- emb_747\n",
      "- emb_748\n",
      "- emb_749\n",
      "- emb_750\n",
      "- emb_751\n",
      "- emb_752\n",
      "- emb_753\n",
      "- emb_754\n",
      "- emb_755\n",
      "- emb_756\n",
      "- emb_757\n",
      "- emb_758\n",
      "- emb_759\n",
      "- emb_760\n",
      "- emb_761\n",
      "- emb_762\n",
      "- emb_763\n",
      "- emb_764\n",
      "- emb_765\n",
      "- emb_766\n",
      "- emb_767\n",
      "- emb_768\n",
      "- emb_769\n",
      "- emb_770\n",
      "- emb_771\n",
      "- emb_772\n",
      "- emb_773\n",
      "- emb_774\n",
      "- emb_775\n",
      "- emb_776\n",
      "- emb_777\n",
      "- emb_778\n",
      "- emb_779\n",
      "- emb_780\n",
      "- emb_781\n",
      "- emb_782\n",
      "- emb_783\n",
      "- emb_784\n",
      "- emb_785\n",
      "- emb_786\n",
      "- emb_787\n",
      "- emb_788\n",
      "- emb_789\n",
      "- emb_790\n",
      "- emb_791\n",
      "- emb_792\n",
      "- emb_793\n",
      "- emb_794\n",
      "- emb_795\n",
      "- emb_796\n",
      "- emb_797\n",
      "- emb_798\n",
      "- emb_799\n",
      "- emb_800\n",
      "- emb_801\n",
      "- emb_802\n",
      "- emb_803\n",
      "- emb_804\n",
      "- emb_805\n",
      "- emb_806\n",
      "- emb_807\n",
      "- emb_808\n",
      "- emb_809\n",
      "- emb_810\n",
      "- emb_811\n",
      "- emb_812\n",
      "- emb_813\n",
      "- emb_814\n",
      "- emb_815\n",
      "- emb_816\n",
      "- emb_817\n",
      "- emb_818\n",
      "- emb_819\n",
      "- emb_820\n",
      "- emb_821\n",
      "- emb_822\n",
      "- emb_823\n",
      "- emb_824\n",
      "- emb_825\n",
      "- emb_826\n",
      "- emb_827\n",
      "- emb_828\n",
      "- emb_829\n",
      "- emb_830\n",
      "- emb_831\n",
      "- emb_832\n",
      "- emb_833\n",
      "- emb_834\n",
      "- emb_835\n",
      "- emb_836\n",
      "- emb_837\n",
      "- emb_838\n",
      "- emb_839\n",
      "- emb_840\n",
      "- emb_841\n",
      "- emb_842\n",
      "- emb_843\n",
      "- emb_844\n",
      "- emb_845\n",
      "- emb_846\n",
      "- emb_847\n",
      "- emb_848\n",
      "- emb_849\n",
      "- emb_850\n",
      "- emb_851\n",
      "- emb_852\n",
      "- emb_853\n",
      "- emb_854\n",
      "- emb_855\n",
      "- emb_856\n",
      "- emb_857\n",
      "- emb_858\n",
      "- emb_859\n",
      "- emb_860\n",
      "- emb_861\n",
      "- emb_862\n",
      "- emb_863\n",
      "- emb_864\n",
      "- emb_865\n",
      "- emb_866\n",
      "- emb_867\n",
      "- emb_868\n",
      "- emb_869\n",
      "- emb_870\n",
      "- emb_871\n",
      "- emb_872\n",
      "- emb_873\n",
      "- emb_874\n",
      "- emb_875\n",
      "- emb_876\n",
      "- emb_877\n",
      "- emb_878\n",
      "- emb_879\n",
      "- emb_880\n",
      "- emb_881\n",
      "- emb_882\n",
      "- emb_883\n",
      "- emb_884\n",
      "- emb_885\n",
      "- emb_886\n",
      "- emb_887\n",
      "- emb_888\n",
      "- emb_889\n",
      "- emb_890\n",
      "- emb_891\n",
      "- emb_892\n",
      "- emb_893\n",
      "- emb_894\n",
      "- emb_895\n",
      "- emb_896\n",
      "- emb_897\n",
      "- emb_898\n",
      "- emb_899\n",
      "- emb_900\n",
      "- emb_901\n",
      "- emb_902\n",
      "- emb_903\n",
      "- emb_904\n",
      "- emb_905\n",
      "- emb_906\n",
      "- emb_907\n",
      "- emb_908\n",
      "- emb_909\n",
      "- emb_910\n",
      "- emb_911\n",
      "- emb_912\n",
      "- emb_913\n",
      "- emb_914\n",
      "- emb_915\n",
      "- emb_916\n",
      "- emb_917\n",
      "- emb_918\n",
      "- emb_919\n",
      "- emb_920\n",
      "- emb_921\n",
      "- emb_922\n",
      "- emb_923\n",
      "- emb_924\n",
      "- emb_925\n",
      "- emb_926\n",
      "- emb_927\n",
      "- emb_928\n",
      "- emb_929\n",
      "- emb_930\n",
      "- emb_931\n",
      "- emb_932\n",
      "- emb_933\n",
      "- emb_934\n",
      "- emb_935\n",
      "- emb_936\n",
      "- emb_937\n",
      "- emb_938\n",
      "- emb_939\n",
      "- emb_940\n",
      "- emb_941\n",
      "- emb_942\n",
      "- emb_943\n",
      "- emb_944\n",
      "- emb_945\n",
      "- emb_946\n",
      "- emb_947\n",
      "- emb_948\n",
      "- emb_949\n",
      "- emb_950\n",
      "- emb_951\n",
      "- emb_952\n",
      "- emb_953\n",
      "- emb_954\n",
      "- emb_955\n",
      "- emb_956\n",
      "- emb_957\n",
      "- emb_958\n",
      "- emb_959\n",
      "- emb_960\n",
      "- emb_961\n",
      "- emb_962\n",
      "- emb_963\n",
      "- emb_964\n",
      "- emb_965\n",
      "- emb_966\n",
      "- emb_967\n",
      "- emb_968\n",
      "- emb_969\n",
      "- emb_970\n",
      "- emb_971\n",
      "- emb_972\n",
      "- emb_973\n",
      "- emb_974\n",
      "- emb_975\n",
      "- emb_976\n",
      "- emb_977\n",
      "- emb_978\n",
      "- emb_979\n",
      "- emb_980\n",
      "- emb_981\n",
      "- emb_982\n",
      "- emb_983\n",
      "- emb_984\n",
      "- emb_985\n",
      "- emb_986\n",
      "- emb_987\n",
      "- emb_988\n",
      "- emb_989\n",
      "- emb_990\n",
      "- emb_991\n",
      "- emb_992\n",
      "- emb_993\n",
      "- emb_994\n",
      "- emb_995\n",
      "- emb_996\n",
      "- emb_997\n",
      "- emb_998\n",
      "- emb_999\n",
      "- emb_1000\n",
      "- emb_1001\n",
      "- emb_1002\n",
      "- emb_1003\n",
      "- emb_1004\n",
      "- emb_1005\n",
      "- emb_1006\n",
      "- emb_1007\n",
      "- emb_1008\n",
      "- emb_1009\n",
      "- emb_1010\n",
      "- emb_1011\n",
      "- emb_1012\n",
      "- emb_1013\n",
      "- emb_1014\n",
      "- emb_1015\n",
      "- emb_1016\n",
      "- emb_1017\n",
      "- emb_1018\n",
      "- emb_1019\n",
      "- emb_1020\n",
      "- emb_1021\n",
      "- emb_1022\n",
      "- emb_1023\n",
      "- emb_1024\n",
      "- emb_1025\n",
      "- emb_1026\n",
      "- emb_1027\n",
      "- emb_1028\n",
      "- emb_1029\n",
      "- emb_1030\n",
      "- emb_1031\n",
      "- emb_1032\n",
      "- emb_1033\n",
      "- emb_1034\n",
      "- emb_1035\n",
      "- emb_1036\n",
      "- emb_1037\n",
      "- emb_1038\n",
      "- emb_1039\n",
      "- emb_1040\n",
      "- emb_1041\n",
      "- emb_1042\n",
      "- emb_1043\n",
      "- emb_1044\n",
      "- emb_1045\n",
      "- emb_1046\n",
      "- emb_1047\n",
      "- emb_1048\n",
      "- emb_1049\n",
      "- emb_1050\n",
      "- emb_1051\n",
      "- emb_1052\n",
      "- emb_1053\n",
      "- emb_1054\n",
      "- emb_1055\n",
      "- emb_1056\n",
      "- emb_1057\n",
      "- emb_1058\n",
      "- emb_1059\n",
      "- emb_1060\n",
      "- emb_1061\n",
      "- emb_1062\n",
      "- emb_1063\n",
      "- emb_1064\n",
      "- emb_1065\n",
      "- emb_1066\n",
      "- emb_1067\n",
      "- emb_1068\n",
      "- emb_1069\n",
      "- emb_1070\n",
      "- emb_1071\n",
      "- emb_1072\n",
      "- emb_1073\n",
      "- emb_1074\n",
      "- emb_1075\n",
      "- emb_1076\n",
      "- emb_1077\n",
      "- emb_1078\n",
      "- emb_1079\n",
      "- emb_1080\n",
      "- emb_1081\n",
      "- emb_1082\n",
      "- emb_1083\n",
      "- emb_1084\n",
      "- emb_1085\n",
      "- emb_1086\n",
      "- emb_1087\n",
      "- emb_1088\n",
      "- emb_1089\n",
      "- emb_1090\n",
      "- emb_1091\n",
      "- emb_1092\n",
      "- emb_1093\n",
      "- emb_1094\n",
      "- emb_1095\n",
      "- emb_1096\n",
      "- emb_1097\n",
      "- emb_1098\n",
      "- emb_1099\n",
      "- emb_1100\n",
      "- emb_1101\n",
      "- emb_1102\n",
      "- emb_1103\n",
      "- emb_1104\n",
      "- emb_1105\n",
      "- emb_1106\n",
      "- emb_1107\n",
      "- emb_1108\n",
      "- emb_1109\n",
      "- emb_1110\n",
      "- emb_1111\n",
      "- emb_1112\n",
      "- emb_1113\n",
      "- emb_1114\n",
      "- emb_1115\n",
      "- emb_1116\n",
      "- emb_1117\n",
      "- emb_1118\n",
      "- emb_1119\n",
      "- emb_1120\n",
      "- emb_1121\n",
      "- emb_1122\n",
      "- emb_1123\n",
      "- emb_1124\n",
      "- emb_1125\n",
      "- emb_1126\n",
      "- emb_1127\n",
      "- emb_1128\n",
      "- emb_1129\n",
      "- emb_1130\n",
      "- emb_1131\n",
      "- emb_1132\n",
      "- emb_1133\n",
      "- emb_1134\n",
      "- emb_1135\n",
      "- emb_1136\n",
      "- emb_1137\n",
      "- emb_1138\n",
      "- emb_1139\n",
      "- emb_1140\n",
      "- emb_1141\n",
      "- emb_1142\n",
      "- emb_1143\n",
      "- emb_1144\n",
      "- emb_1145\n",
      "- emb_1146\n",
      "- emb_1147\n",
      "- emb_1148\n",
      "- emb_1149\n",
      "- emb_1150\n",
      "- emb_1151\n",
      "- emb_1152\n",
      "- emb_1153\n",
      "- emb_1154\n",
      "- emb_1155\n",
      "- emb_1156\n",
      "- emb_1157\n",
      "- emb_1158\n",
      "- emb_1159\n",
      "- emb_1160\n",
      "- emb_1161\n",
      "- emb_1162\n",
      "- emb_1163\n",
      "- emb_1164\n",
      "- emb_1165\n",
      "- emb_1166\n",
      "- emb_1167\n",
      "- emb_1168\n",
      "- emb_1169\n",
      "- emb_1170\n",
      "- emb_1171\n",
      "- emb_1172\n",
      "- emb_1173\n",
      "- emb_1174\n",
      "- emb_1175\n",
      "- emb_1176\n",
      "- emb_1177\n",
      "- emb_1178\n",
      "- emb_1179\n",
      "- emb_1180\n",
      "- emb_1181\n",
      "- emb_1182\n",
      "- emb_1183\n",
      "- emb_1184\n",
      "- emb_1185\n",
      "- emb_1186\n",
      "- emb_1187\n",
      "- emb_1188\n",
      "- emb_1189\n",
      "- emb_1190\n",
      "- emb_1191\n",
      "- emb_1192\n",
      "- emb_1193\n",
      "- emb_1194\n",
      "- emb_1195\n",
      "- emb_1196\n",
      "- emb_1197\n",
      "- emb_1198\n",
      "- emb_1199\n",
      "- emb_1200\n",
      "- emb_1201\n",
      "- emb_1202\n",
      "- emb_1203\n",
      "- emb_1204\n",
      "- emb_1205\n",
      "- emb_1206\n",
      "- emb_1207\n",
      "- emb_1208\n",
      "- emb_1209\n",
      "- emb_1210\n",
      "- emb_1211\n",
      "- emb_1212\n",
      "- emb_1213\n",
      "- emb_1214\n",
      "- emb_1215\n",
      "- emb_1216\n",
      "- emb_1217\n",
      "- emb_1218\n",
      "- emb_1219\n",
      "- emb_1220\n",
      "- emb_1221\n",
      "- emb_1222\n",
      "- emb_1223\n",
      "- emb_1224\n",
      "- emb_1225\n",
      "- emb_1226\n",
      "- emb_1227\n",
      "- emb_1228\n",
      "- emb_1229\n",
      "- emb_1230\n",
      "- emb_1231\n",
      "- emb_1232\n",
      "- emb_1233\n",
      "- emb_1234\n",
      "- emb_1235\n",
      "- emb_1236\n",
      "- emb_1237\n",
      "- emb_1238\n",
      "- emb_1239\n",
      "- emb_1240\n",
      "- emb_1241\n",
      "- emb_1242\n",
      "- emb_1243\n",
      "- emb_1244\n",
      "- emb_1245\n",
      "- emb_1246\n",
      "- emb_1247\n",
      "- emb_1248\n",
      "- emb_1249\n",
      "- emb_1250\n",
      "- emb_1251\n",
      "- emb_1252\n",
      "- emb_1253\n",
      "- emb_1254\n",
      "- emb_1255\n",
      "- emb_1256\n",
      "- emb_1257\n",
      "- emb_1258\n",
      "- emb_1259\n",
      "- emb_1260\n",
      "- emb_1261\n",
      "- emb_1262\n",
      "- emb_1263\n",
      "- emb_1264\n",
      "- emb_1265\n",
      "- emb_1266\n",
      "- emb_1267\n",
      "- emb_1268\n",
      "- emb_1269\n",
      "- emb_1270\n",
      "- emb_1271\n",
      "- emb_1272\n",
      "- emb_1273\n",
      "- emb_1274\n",
      "- emb_1275\n",
      "- emb_1276\n",
      "- emb_1277\n",
      "- emb_1278\n",
      "- emb_1279\n",
      "- emb_1280\n",
      "- emb_1281\n",
      "- emb_1282\n",
      "- emb_1283\n",
      "- emb_1284\n",
      "- emb_1285\n",
      "- emb_1286\n",
      "- emb_1287\n",
      "- emb_1288\n",
      "- emb_1289\n",
      "- emb_1290\n",
      "- emb_1291\n",
      "- emb_1292\n",
      "- emb_1293\n",
      "- emb_1294\n",
      "- emb_1295\n",
      "- emb_1296\n",
      "- emb_1297\n",
      "- emb_1298\n",
      "- emb_1299\n",
      "- emb_1300\n",
      "- emb_1301\n",
      "- emb_1302\n",
      "- emb_1303\n",
      "- emb_1304\n",
      "- emb_1305\n",
      "- emb_1306\n",
      "- emb_1307\n",
      "- emb_1308\n",
      "- emb_1309\n",
      "- emb_1310\n",
      "- emb_1311\n",
      "- emb_1312\n",
      "- emb_1313\n",
      "- emb_1314\n",
      "- emb_1315\n",
      "- emb_1316\n",
      "- emb_1317\n",
      "- emb_1318\n",
      "- emb_1319\n",
      "- emb_1320\n",
      "- emb_1321\n",
      "- emb_1322\n",
      "- emb_1323\n",
      "- emb_1324\n",
      "- emb_1325\n",
      "- emb_1326\n",
      "- emb_1327\n",
      "- emb_1328\n",
      "- emb_1329\n",
      "- emb_1330\n",
      "- emb_1331\n",
      "- emb_1332\n",
      "- emb_1333\n",
      "- emb_1334\n",
      "- emb_1335\n",
      "- emb_1336\n",
      "- emb_1337\n",
      "- emb_1338\n",
      "- emb_1339\n",
      "- emb_1340\n",
      "- emb_1341\n",
      "- emb_1342\n",
      "- emb_1343\n",
      "- emb_1344\n",
      "- emb_1345\n",
      "- emb_1346\n",
      "- emb_1347\n",
      "- emb_1348\n",
      "- emb_1349\n",
      "- emb_1350\n",
      "- emb_1351\n",
      "- emb_1352\n",
      "- emb_1353\n",
      "- emb_1354\n",
      "- emb_1355\n",
      "- emb_1356\n",
      "- emb_1357\n",
      "- emb_1358\n",
      "- emb_1359\n",
      "- emb_1360\n",
      "- emb_1361\n",
      "- emb_1362\n",
      "- emb_1363\n",
      "- emb_1364\n",
      "- emb_1365\n",
      "- emb_1366\n",
      "- emb_1367\n",
      "- emb_1368\n",
      "- emb_1369\n",
      "- emb_1370\n",
      "- emb_1371\n",
      "- emb_1372\n",
      "- emb_1373\n",
      "- emb_1374\n",
      "- emb_1375\n",
      "- emb_1376\n",
      "- emb_1377\n",
      "- emb_1378\n",
      "- emb_1379\n",
      "- emb_1380\n",
      "- emb_1381\n",
      "- emb_1382\n",
      "- emb_1383\n",
      "- emb_1384\n",
      "- emb_1385\n",
      "- emb_1386\n",
      "- emb_1387\n",
      "- emb_1388\n",
      "- emb_1389\n",
      "- emb_1390\n",
      "- emb_1391\n",
      "- emb_1392\n",
      "- emb_1393\n",
      "- emb_1394\n",
      "- emb_1395\n",
      "- emb_1396\n",
      "- emb_1397\n",
      "- emb_1398\n",
      "- emb_1399\n",
      "- emb_1400\n",
      "- emb_1401\n",
      "- emb_1402\n",
      "- emb_1403\n",
      "- emb_1404\n",
      "- emb_1405\n",
      "- emb_1406\n",
      "- emb_1407\n",
      "- emb_1408\n",
      "- emb_1409\n",
      "- emb_1410\n",
      "- emb_1411\n",
      "- emb_1412\n",
      "- emb_1413\n",
      "- emb_1414\n",
      "- emb_1415\n",
      "- emb_1416\n",
      "- emb_1417\n",
      "- emb_1418\n",
      "- emb_1419\n",
      "- emb_1420\n",
      "- emb_1421\n",
      "- emb_1422\n",
      "- emb_1423\n",
      "- emb_1424\n",
      "- emb_1425\n",
      "- emb_1426\n",
      "- emb_1427\n",
      "- emb_1428\n",
      "- emb_1429\n",
      "- emb_1430\n",
      "- emb_1431\n",
      "- emb_1432\n",
      "- emb_1433\n",
      "- emb_1434\n",
      "- emb_1435\n",
      "- emb_1436\n",
      "- emb_1437\n",
      "- emb_1438\n",
      "- emb_1439\n",
      "- emb_1440\n",
      "- emb_1441\n",
      "- emb_1442\n",
      "- emb_1443\n",
      "- emb_1444\n",
      "- emb_1445\n",
      "- emb_1446\n",
      "- emb_1447\n",
      "- emb_1448\n",
      "- emb_1449\n",
      "- emb_1450\n",
      "- emb_1451\n",
      "- emb_1452\n",
      "- emb_1453\n",
      "- emb_1454\n",
      "- emb_1455\n",
      "- emb_1456\n",
      "- emb_1457\n",
      "- emb_1458\n",
      "- emb_1459\n",
      "- emb_1460\n",
      "- emb_1461\n",
      "- emb_1462\n",
      "- emb_1463\n",
      "- emb_1464\n",
      "- emb_1465\n",
      "- emb_1466\n",
      "- emb_1467\n",
      "- emb_1468\n",
      "- emb_1469\n",
      "- emb_1470\n",
      "- emb_1471\n",
      "- emb_1472\n",
      "- emb_1473\n",
      "- emb_1474\n",
      "- emb_1475\n",
      "- emb_1476\n",
      "- emb_1477\n",
      "- emb_1478\n",
      "- emb_1479\n",
      "- emb_1480\n",
      "- emb_1481\n",
      "- emb_1482\n",
      "- emb_1483\n",
      "- emb_1484\n",
      "- emb_1485\n",
      "- emb_1486\n",
      "- emb_1487\n",
      "- emb_1488\n",
      "- emb_1489\n",
      "- emb_1490\n",
      "- emb_1491\n",
      "- emb_1492\n",
      "- emb_1493\n",
      "- emb_1494\n",
      "- emb_1495\n",
      "- emb_1496\n",
      "- emb_1497\n",
      "- emb_1498\n",
      "- emb_1499\n",
      "- emb_1500\n",
      "- emb_1501\n",
      "- emb_1502\n",
      "- emb_1503\n",
      "- emb_1504\n",
      "- emb_1505\n",
      "- emb_1506\n",
      "- emb_1507\n",
      "- emb_1508\n",
      "- emb_1509\n",
      "- emb_1510\n",
      "- emb_1511\n",
      "- emb_1512\n",
      "- emb_1513\n",
      "- emb_1514\n",
      "- emb_1515\n",
      "- emb_1516\n",
      "- emb_1517\n",
      "- emb_1518\n",
      "- emb_1519\n",
      "- emb_1520\n",
      "- emb_1521\n",
      "- emb_1522\n",
      "- emb_1523\n",
      "- emb_1524\n",
      "- emb_1525\n",
      "- emb_1526\n",
      "- emb_1527\n",
      "- emb_1528\n",
      "- emb_1529\n",
      "- emb_1530\n",
      "- emb_1531\n",
      "- emb_1532\n",
      "- emb_1533\n",
      "- emb_1534\n",
      "- emb_1535\n",
      "- emb_1536\n",
      "- emb_1537\n",
      "- emb_1538\n",
      "- emb_1539\n",
      "- emb_1540\n",
      "- emb_1541\n",
      "- emb_1542\n",
      "- emb_1543\n",
      "- emb_1544\n",
      "- emb_1545\n",
      "- emb_1546\n",
      "- emb_1547\n",
      "- emb_1548\n",
      "- emb_1549\n",
      "- emb_1550\n",
      "- emb_1551\n",
      "- emb_1552\n",
      "- emb_1553\n",
      "- emb_1554\n",
      "- emb_1555\n",
      "- emb_1556\n",
      "- emb_1557\n",
      "- emb_1558\n",
      "- emb_1559\n",
      "- emb_1560\n",
      "- emb_1561\n",
      "- emb_1562\n",
      "- emb_1563\n",
      "- emb_1564\n",
      "- emb_1565\n",
      "- emb_1566\n",
      "- emb_1567\n",
      "- emb_1568\n",
      "- emb_1569\n",
      "- emb_1570\n",
      "- emb_1571\n",
      "- emb_1572\n",
      "- emb_1573\n",
      "- emb_1574\n",
      "- emb_1575\n",
      "- emb_1576\n",
      "- emb_1577\n",
      "- emb_1578\n",
      "- emb_1579\n",
      "- emb_1580\n",
      "- emb_1581\n",
      "- emb_1582\n",
      "- emb_1583\n",
      "- emb_1584\n",
      "- emb_1585\n",
      "- emb_1586\n",
      "- emb_1587\n",
      "- emb_1588\n",
      "- emb_1589\n",
      "- emb_1590\n",
      "- emb_1591\n",
      "- emb_1592\n",
      "- emb_1593\n",
      "- emb_1594\n",
      "- emb_1595\n",
      "- emb_1596\n",
      "- emb_1597\n",
      "- emb_1598\n",
      "- emb_1599\n",
      "- emb_1600\n",
      "- emb_1601\n",
      "- emb_1602\n",
      "- emb_1603\n",
      "- emb_1604\n",
      "- emb_1605\n",
      "- emb_1606\n",
      "- emb_1607\n",
      "- emb_1608\n",
      "- emb_1609\n",
      "- emb_1610\n",
      "- emb_1611\n",
      "- emb_1612\n",
      "- emb_1613\n",
      "- emb_1614\n",
      "- emb_1615\n",
      "- emb_1616\n",
      "- emb_1617\n",
      "- emb_1618\n",
      "- emb_1619\n",
      "- emb_1620\n",
      "- emb_1621\n",
      "- emb_1622\n",
      "- emb_1623\n",
      "- emb_1624\n",
      "- emb_1625\n",
      "- emb_1626\n",
      "- emb_1627\n",
      "- emb_1628\n",
      "- emb_1629\n",
      "- emb_1630\n",
      "- emb_1631\n",
      "- emb_1632\n",
      "- emb_1633\n",
      "- emb_1634\n",
      "- emb_1635\n",
      "- emb_1636\n",
      "- emb_1637\n",
      "- emb_1638\n",
      "- emb_1639\n",
      "- emb_1640\n",
      "- emb_1641\n",
      "- emb_1642\n",
      "- emb_1643\n",
      "- emb_1644\n",
      "- emb_1645\n",
      "- emb_1646\n",
      "- emb_1647\n",
      "- emb_1648\n",
      "- emb_1649\n",
      "- emb_1650\n",
      "- emb_1651\n",
      "- emb_1652\n",
      "- emb_1653\n",
      "- emb_1654\n",
      "- emb_1655\n",
      "- emb_1656\n",
      "- emb_1657\n",
      "- emb_1658\n",
      "- emb_1659\n",
      "- emb_1660\n",
      "- emb_1661\n",
      "- emb_1662\n",
      "- emb_1663\n",
      "- emb_1664\n",
      "- emb_1665\n",
      "- emb_1666\n",
      "- emb_1667\n",
      "- emb_1668\n",
      "- emb_1669\n",
      "- emb_1670\n",
      "- emb_1671\n",
      "- emb_1672\n",
      "- emb_1673\n",
      "- emb_1674\n",
      "- emb_1675\n",
      "- emb_1676\n",
      "- emb_1677\n",
      "- emb_1678\n",
      "- emb_1679\n",
      "- emb_1680\n",
      "- emb_1681\n",
      "- emb_1682\n",
      "- emb_1683\n",
      "- emb_1684\n",
      "- emb_1685\n",
      "- emb_1686\n",
      "- emb_1687\n",
      "- emb_1688\n",
      "- emb_1689\n",
      "- emb_1690\n",
      "- emb_1691\n",
      "- emb_1692\n",
      "- emb_1693\n",
      "- emb_1694\n",
      "- emb_1695\n",
      "- emb_1696\n",
      "- emb_1697\n",
      "- emb_1698\n",
      "- emb_1699\n",
      "- emb_1700\n",
      "- emb_1701\n",
      "- emb_1702\n",
      "- emb_1703\n",
      "- emb_1704\n",
      "- emb_1705\n",
      "- emb_1706\n",
      "- emb_1707\n",
      "- emb_1708\n",
      "- emb_1709\n",
      "- emb_1710\n",
      "- emb_1711\n",
      "- emb_1712\n",
      "- emb_1713\n",
      "- emb_1714\n",
      "- emb_1715\n",
      "- emb_1716\n",
      "- emb_1717\n",
      "- emb_1718\n",
      "- emb_1719\n",
      "- emb_1720\n",
      "- emb_1721\n",
      "- emb_1722\n",
      "- emb_1723\n",
      "- emb_1724\n",
      "- emb_1725\n",
      "- emb_1726\n",
      "- emb_1727\n",
      "- emb_1728\n",
      "- emb_1729\n",
      "- emb_1730\n",
      "- emb_1731\n",
      "- emb_1732\n",
      "- emb_1733\n",
      "- emb_1734\n",
      "- emb_1735\n",
      "- emb_1736\n",
      "- emb_1737\n",
      "- emb_1738\n",
      "- emb_1739\n",
      "- emb_1740\n",
      "- emb_1741\n",
      "- emb_1742\n",
      "- emb_1743\n",
      "- emb_1744\n",
      "- emb_1745\n",
      "- emb_1746\n",
      "- emb_1747\n",
      "- emb_1748\n",
      "- emb_1749\n",
      "- emb_1750\n",
      "- emb_1751\n",
      "- emb_1752\n",
      "- emb_1753\n",
      "- emb_1754\n",
      "- emb_1755\n",
      "- emb_1756\n",
      "- emb_1757\n",
      "- emb_1758\n",
      "- emb_1759\n",
      "- emb_1760\n",
      "- emb_1761\n",
      "- emb_1762\n",
      "- emb_1763\n",
      "- emb_1764\n",
      "- emb_1765\n",
      "- emb_1766\n",
      "- emb_1767\n",
      "- emb_1768\n",
      "- emb_1769\n",
      "- emb_1770\n",
      "- emb_1771\n",
      "- emb_1772\n",
      "- emb_1773\n",
      "- emb_1774\n",
      "- emb_1775\n",
      "- emb_1776\n",
      "- emb_1777\n",
      "- emb_1778\n",
      "- emb_1779\n",
      "- emb_1780\n",
      "- emb_1781\n",
      "- emb_1782\n",
      "- emb_1783\n",
      "- emb_1784\n",
      "- emb_1785\n",
      "- emb_1786\n",
      "- emb_1787\n",
      "- emb_1788\n",
      "- emb_1789\n",
      "- emb_1790\n",
      "- emb_1791\n",
      "- emb_1792\n",
      "- emb_1793\n",
      "- emb_1794\n",
      "- emb_1795\n",
      "- emb_1796\n",
      "- emb_1797\n",
      "- emb_1798\n",
      "- emb_1799\n",
      "- emb_1800\n",
      "- emb_1801\n",
      "- emb_1802\n",
      "- emb_1803\n",
      "- emb_1804\n",
      "- emb_1805\n",
      "- emb_1806\n",
      "- emb_1807\n",
      "- emb_1808\n",
      "- emb_1809\n",
      "- emb_1810\n",
      "- emb_1811\n",
      "- emb_1812\n",
      "- emb_1813\n",
      "- emb_1814\n",
      "- emb_1815\n",
      "- emb_1816\n",
      "- emb_1817\n",
      "- emb_1818\n",
      "- emb_1819\n",
      "- emb_1820\n",
      "- emb_1821\n",
      "- emb_1822\n",
      "- emb_1823\n",
      "- emb_1824\n",
      "- emb_1825\n",
      "- emb_1826\n",
      "- emb_1827\n",
      "- emb_1828\n",
      "- emb_1829\n",
      "- emb_1830\n",
      "- emb_1831\n",
      "- emb_1832\n",
      "- emb_1833\n",
      "- emb_1834\n",
      "- emb_1835\n",
      "- emb_1836\n",
      "- emb_1837\n",
      "- emb_1838\n",
      "- emb_1839\n",
      "- emb_1840\n",
      "- emb_1841\n",
      "- emb_1842\n",
      "- emb_1843\n",
      "- emb_1844\n",
      "- emb_1845\n",
      "- emb_1846\n",
      "- emb_1847\n",
      "- emb_1848\n",
      "- emb_1849\n",
      "- emb_1850\n",
      "- emb_1851\n",
      "- emb_1852\n",
      "- emb_1853\n",
      "- emb_1854\n",
      "- emb_1855\n",
      "- emb_1856\n",
      "- emb_1857\n",
      "- emb_1858\n",
      "- emb_1859\n",
      "- emb_1860\n",
      "- emb_1861\n",
      "- emb_1862\n",
      "- emb_1863\n",
      "- emb_1864\n",
      "- emb_1865\n",
      "- emb_1866\n",
      "- emb_1867\n",
      "- emb_1868\n",
      "- emb_1869\n",
      "- emb_1870\n",
      "- emb_1871\n",
      "- emb_1872\n",
      "- emb_1873\n",
      "- emb_1874\n",
      "- emb_1875\n",
      "- emb_1876\n",
      "- emb_1877\n",
      "- emb_1878\n",
      "- emb_1879\n",
      "- emb_1880\n",
      "- emb_1881\n",
      "- emb_1882\n",
      "- emb_1883\n",
      "- emb_1884\n",
      "- emb_1885\n",
      "- emb_1886\n",
      "- emb_1887\n",
      "- emb_1888\n",
      "- emb_1889\n",
      "- emb_1890\n",
      "- emb_1891\n",
      "- emb_1892\n",
      "- emb_1893\n",
      "- emb_1894\n",
      "- emb_1895\n",
      "- emb_1896\n",
      "- emb_1897\n",
      "- emb_1898\n",
      "- emb_1899\n",
      "- emb_1900\n",
      "- emb_1901\n",
      "- emb_1902\n",
      "- emb_1903\n",
      "- emb_1904\n",
      "- emb_1905\n",
      "- emb_1906\n",
      "- emb_1907\n",
      "- emb_1908\n",
      "- emb_1909\n",
      "- emb_1910\n",
      "- emb_1911\n",
      "- emb_1912\n",
      "- emb_1913\n",
      "- emb_1914\n",
      "- emb_1915\n",
      "- emb_1916\n",
      "- emb_1917\n",
      "- emb_1918\n",
      "- emb_1919\n",
      "- emb_1920\n",
      "- emb_1921\n",
      "- emb_1922\n",
      "- emb_1923\n",
      "- emb_1924\n",
      "- emb_1925\n",
      "- emb_1926\n",
      "- emb_1927\n",
      "- emb_1928\n",
      "- emb_1929\n",
      "- emb_1930\n",
      "- emb_1931\n",
      "- emb_1932\n",
      "- emb_1933\n",
      "- emb_1934\n",
      "- emb_1935\n",
      "- emb_1936\n",
      "- emb_1937\n",
      "- emb_1938\n",
      "- emb_1939\n",
      "- emb_1940\n",
      "- emb_1941\n",
      "- emb_1942\n",
      "- emb_1943\n",
      "- emb_1944\n",
      "- emb_1945\n",
      "- emb_1946\n",
      "- emb_1947\n",
      "- emb_1948\n",
      "- emb_1949\n",
      "- emb_1950\n",
      "- emb_1951\n",
      "- emb_1952\n",
      "- emb_1953\n",
      "- emb_1954\n",
      "- emb_1955\n",
      "- emb_1956\n",
      "- emb_1957\n",
      "- emb_1958\n",
      "- emb_1959\n",
      "- emb_1960\n",
      "- emb_1961\n",
      "- emb_1962\n",
      "- emb_1963\n",
      "- emb_1964\n",
      "- emb_1965\n",
      "- emb_1966\n",
      "- emb_1967\n",
      "- emb_1968\n",
      "- emb_1969\n",
      "- emb_1970\n",
      "- emb_1971\n",
      "- emb_1972\n",
      "- emb_1973\n",
      "- emb_1974\n",
      "- emb_1975\n",
      "- emb_1976\n",
      "- emb_1977\n",
      "- emb_1978\n",
      "- emb_1979\n",
      "- emb_1980\n",
      "- emb_1981\n",
      "- emb_1982\n",
      "- emb_1983\n",
      "- emb_1984\n",
      "- emb_1985\n",
      "- emb_1986\n",
      "- emb_1987\n",
      "- emb_1988\n",
      "- emb_1989\n",
      "- emb_1990\n",
      "- emb_1991\n",
      "- emb_1992\n",
      "- emb_1993\n",
      "- emb_1994\n",
      "- emb_1995\n",
      "- emb_1996\n",
      "- emb_1997\n",
      "- emb_1998\n",
      "- emb_1999\n",
      "- emb_2000\n",
      "- emb_2001\n",
      "- emb_2002\n",
      "- emb_2003\n",
      "- emb_2004\n",
      "- emb_2005\n",
      "- emb_2006\n",
      "- emb_2007\n",
      "- emb_2008\n",
      "- emb_2009\n",
      "- emb_2010\n",
      "- emb_2011\n",
      "- emb_2012\n",
      "- emb_2013\n",
      "- emb_2014\n",
      "- emb_2015\n",
      "- emb_2016\n",
      "- emb_2017\n",
      "- emb_2018\n",
      "- emb_2019\n",
      "- emb_2020\n",
      "- emb_2021\n",
      "- emb_2022\n",
      "- emb_2023\n",
      "- emb_2024\n",
      "- emb_2025\n",
      "- emb_2026\n",
      "- emb_2027\n",
      "- emb_2028\n",
      "- emb_2029\n",
      "- emb_2030\n",
      "- emb_2031\n",
      "- emb_2032\n",
      "- emb_2033\n",
      "- emb_2034\n",
      "- emb_2035\n",
      "- emb_2036\n",
      "- emb_2037\n",
      "- emb_2038\n",
      "- emb_2039\n",
      "- emb_2040\n",
      "- emb_2041\n",
      "- emb_2042\n",
      "- emb_2043\n",
      "- emb_2044\n",
      "- emb_2045\n",
      "- emb_2046\n",
      "- emb_2047\n",
      "- emb_2048\n",
      "- emb_2049\n",
      "- emb_2050\n",
      "- emb_2051\n",
      "- emb_2052\n",
      "- emb_2053\n",
      "- emb_2054\n",
      "- emb_2055\n",
      "- emb_2056\n",
      "- emb_2057\n",
      "- emb_2058\n",
      "- emb_2059\n",
      "- emb_2060\n",
      "- emb_2061\n",
      "- emb_2062\n",
      "- emb_2063\n",
      "- emb_2064\n",
      "- emb_2065\n",
      "- emb_2066\n",
      "- emb_2067\n",
      "- emb_2068\n",
      "- emb_2069\n",
      "- emb_2070\n",
      "- emb_2071\n",
      "- emb_2072\n",
      "- emb_2073\n",
      "- emb_2074\n",
      "- emb_2075\n",
      "- emb_2076\n",
      "- emb_2077\n",
      "- emb_2078\n",
      "- emb_2079\n",
      "- emb_2080\n",
      "- emb_2081\n",
      "- emb_2082\n",
      "- emb_2083\n",
      "- emb_2084\n",
      "- emb_2085\n",
      "- emb_2086\n",
      "- emb_2087\n",
      "- emb_2088\n",
      "- emb_2089\n",
      "- emb_2090\n",
      "- emb_2091\n",
      "- emb_2092\n",
      "- emb_2093\n",
      "- emb_2094\n",
      "- emb_2095\n",
      "- emb_2096\n",
      "- emb_2097\n",
      "- emb_2098\n",
      "- emb_2099\n",
      "- emb_2100\n",
      "- emb_2101\n",
      "- emb_2102\n",
      "- emb_2103\n",
      "- emb_2104\n",
      "- emb_2105\n",
      "- emb_2106\n",
      "- emb_2107\n",
      "- emb_2108\n",
      "- emb_2109\n",
      "- emb_2110\n",
      "- emb_2111\n",
      "- emb_2112\n",
      "- emb_2113\n",
      "- emb_2114\n",
      "- emb_2115\n",
      "- emb_2116\n",
      "- emb_2117\n",
      "- emb_2118\n",
      "- emb_2119\n",
      "- emb_2120\n",
      "- emb_2121\n",
      "- emb_2122\n",
      "- emb_2123\n",
      "- emb_2124\n",
      "- emb_2125\n",
      "- emb_2126\n",
      "- emb_2127\n",
      "- emb_2128\n",
      "- emb_2129\n",
      "- emb_2130\n",
      "- emb_2131\n",
      "- emb_2132\n",
      "- emb_2133\n",
      "- emb_2134\n",
      "- emb_2135\n",
      "- emb_2136\n",
      "- emb_2137\n",
      "- emb_2138\n",
      "- emb_2139\n",
      "- emb_2140\n",
      "- emb_2141\n",
      "- emb_2142\n",
      "- emb_2143\n",
      "- emb_2144\n",
      "- emb_2145\n",
      "- emb_2146\n",
      "- emb_2147\n",
      "- emb_2148\n",
      "- emb_2149\n",
      "- emb_2150\n",
      "- emb_2151\n",
      "- emb_2152\n",
      "- emb_2153\n",
      "- emb_2154\n",
      "- emb_2155\n",
      "- emb_2156\n",
      "- emb_2157\n",
      "- emb_2158\n",
      "- emb_2159\n",
      "- emb_2160\n",
      "- emb_2161\n",
      "- emb_2162\n",
      "- emb_2163\n",
      "- emb_2164\n",
      "- emb_2165\n",
      "- emb_2166\n",
      "- emb_2167\n",
      "- emb_2168\n",
      "- emb_2169\n",
      "- emb_2170\n",
      "- emb_2171\n",
      "- emb_2172\n",
      "- emb_2173\n",
      "- emb_2174\n",
      "- emb_2175\n",
      "- emb_2176\n",
      "- emb_2177\n",
      "- emb_2178\n",
      "- emb_2179\n",
      "- emb_2180\n",
      "- emb_2181\n",
      "- emb_2182\n",
      "- emb_2183\n",
      "- emb_2184\n",
      "- emb_2185\n",
      "- emb_2186\n",
      "- emb_2187\n",
      "- emb_2188\n",
      "- emb_2189\n",
      "- emb_2190\n",
      "- emb_2191\n",
      "- emb_2192\n",
      "- emb_2193\n",
      "- emb_2194\n",
      "- emb_2195\n",
      "- emb_2196\n",
      "- emb_2197\n",
      "- emb_2198\n",
      "- emb_2199\n",
      "- emb_2200\n",
      "- emb_2201\n",
      "- emb_2202\n",
      "- emb_2203\n",
      "- emb_2204\n",
      "- emb_2205\n",
      "- emb_2206\n",
      "- emb_2207\n",
      "- emb_2208\n",
      "- emb_2209\n",
      "- emb_2210\n",
      "- emb_2211\n",
      "- emb_2212\n",
      "- emb_2213\n",
      "- emb_2214\n",
      "- emb_2215\n",
      "- emb_2216\n",
      "- emb_2217\n",
      "- emb_2218\n",
      "- emb_2219\n",
      "- emb_2220\n",
      "- emb_2221\n",
      "- emb_2222\n",
      "- emb_2223\n",
      "- emb_2224\n",
      "- emb_2225\n",
      "- emb_2226\n",
      "- emb_2227\n",
      "- emb_2228\n",
      "- emb_2229\n",
      "- emb_2230\n",
      "- emb_2231\n",
      "- emb_2232\n",
      "- emb_2233\n",
      "- emb_2234\n",
      "- emb_2235\n",
      "- emb_2236\n",
      "- emb_2237\n",
      "- emb_2238\n",
      "- emb_2239\n",
      "- emb_2240\n",
      "- emb_2241\n",
      "- emb_2242\n",
      "- emb_2243\n",
      "- emb_2244\n",
      "- emb_2245\n",
      "- emb_2246\n",
      "- emb_2247\n",
      "- emb_2248\n",
      "- emb_2249\n",
      "- emb_2250\n",
      "- emb_2251\n",
      "- emb_2252\n",
      "- emb_2253\n",
      "- emb_2254\n",
      "- emb_2255\n",
      "- emb_2256\n",
      "- emb_2257\n",
      "- emb_2258\n",
      "- emb_2259\n",
      "- emb_2260\n",
      "- emb_2261\n",
      "- emb_2262\n",
      "- emb_2263\n",
      "- emb_2264\n",
      "- emb_2265\n",
      "- emb_2266\n",
      "- emb_2267\n",
      "- emb_2268\n",
      "- emb_2269\n",
      "- emb_2270\n",
      "- emb_2271\n",
      "- emb_2272\n",
      "- emb_2273\n",
      "- emb_2274\n",
      "- emb_2275\n",
      "- emb_2276\n",
      "- emb_2277\n",
      "- emb_2278\n",
      "- emb_2279\n",
      "- emb_2280\n",
      "- emb_2281\n",
      "- emb_2282\n",
      "- emb_2283\n",
      "- emb_2284\n",
      "- emb_2285\n",
      "- emb_2286\n",
      "- emb_2287\n",
      "- emb_2288\n",
      "- emb_2289\n",
      "- emb_2290\n",
      "- emb_2291\n",
      "- emb_2292\n",
      "- emb_2293\n",
      "- emb_2294\n",
      "- emb_2295\n",
      "- emb_2296\n",
      "- emb_2297\n",
      "- emb_2298\n",
      "- emb_2299\n",
      "- emb_2300\n",
      "- emb_2301\n",
      "- emb_2302\n",
      "- emb_2303\n"
     ]
    }
   ],
   "source": [
    "small_train_path = \"./424_F2024_Final_PC_small_train_v1.csv\"\n",
    "large_train_path = \"./424_F2024_Final_PC_large_train_v1.csv\"\n",
    "test_path = \"./424_F2024_Final_PC_test_without_response_v1.csv\"\n",
    "\n",
    "X_train, X_val, y_train, y_val, X_test = load_and_preprocess_data(\n",
    "    small_train_path, large_train_path, test_path\n",
    ")\n",
    "\n",
    "print(\"\\nProcessed data shapes:\")\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_val: {X_val.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "print(f\"y_val: {y_val.shape}\")\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "\n",
    "print(\"\\nFeatures created:\")\n",
    "for col in X_train.columns:\n",
    "    print(f\"- {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c3bb26-1d2a-4bdd-b673-130e62e4a608",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "class TextPathway(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.pathway = nn.Sequential(\n",
    "            nn.Linear(input_dim, 300),  # Embedding-like layer\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(300, 100),  # Bidirectional-like transformation\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(100, 64),  # Dense reduction\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.pathway(x)\n",
    "\n",
    "class RatingPredictor(nn.Module):\n",
    "    def __init__(self, bert_dim, num_categorical):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Separate pathways for each text type\n",
    "        self.pros_pathway = TextPathway(bert_dim)\n",
    "        self.cons_pathway = TextPathway(bert_dim)\n",
    "        self.headline_pathway = TextPathway(bert_dim)\n",
    "        \n",
    "        # Pathway for categorical features\n",
    "        self.categorical_pathway = nn.Sequential(\n",
    "            nn.Linear(num_categorical, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "        # Combined processing\n",
    "        combined_dim = 64 * 3 + 32  # Three text pathways + categorical\n",
    "        \n",
    "        self.final_layers = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(64, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid()  # Scale to [0,1], will be rescaled to [1,5]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Split input into components\n",
    "        bert_dim = x.shape[1] - self.categorical_pathway[0].in_features\n",
    "        dim_per_text = bert_dim // 3\n",
    "        \n",
    "        pros_emb = x[:, :dim_per_text]\n",
    "        cons_emb = x[:, dim_per_text:2*dim_per_text]\n",
    "        headline_emb = x[:, 2*dim_per_text:bert_dim]\n",
    "        categorical = x[:, bert_dim:]\n",
    "        \n",
    "        # Process each pathway\n",
    "        pros_features = self.pros_pathway(pros_emb)\n",
    "        cons_features = self.cons_pathway(cons_emb)\n",
    "        headline_features = self.headline_pathway(headline_emb)\n",
    "        categorical_features = self.categorical_pathway(categorical)\n",
    "        \n",
    "        # Combine all features\n",
    "        combined = torch.cat([\n",
    "            pros_features,\n",
    "            cons_features,\n",
    "            headline_features,\n",
    "            categorical_features\n",
    "        ], dim=1)\n",
    "        \n",
    "        # Final processing\n",
    "        output = self.final_layers(combined)\n",
    "        \n",
    "        # Scale from [0,1] to [1,5]\n",
    "        return 1 + 4 * output.squeeze()\n",
    "\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = torch.FloatTensor(X.values)\n",
    "        self.y = torch.FloatTensor(y.values) if y is not None else None\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is not None:\n",
    "            return self.X[idx], self.y[idx]\n",
    "        return self.X[idx]\n",
    "\n",
    "class DeepLearningTrainer:\n",
    "    def __init__(self, bert_dim, num_categorical, device=None):\n",
    "        if device is None:\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        else:\n",
    "            self.device = device\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        self.bert_dim = bert_dim\n",
    "        self.num_categorical = num_categorical\n",
    "    \n",
    "    def train_epoch(self, model, train_loader, criterion, optimizer):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        pbar = tqdm(train_loader, desc=\"Training\")\n",
    "        \n",
    "        for X_batch, y_batch in pbar:\n",
    "            X_batch, y_batch = X_batch.to(self.device), y_batch.to(self.device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "        \n",
    "        return total_loss / len(train_loader)\n",
    "    \n",
    "    def train_and_evaluate(self, X_train, X_val, y_train, y_val, \n",
    "                          batch_size=64, epochs=50, patience=5):\n",
    "        # Create datasets\n",
    "        train_dataset = ReviewDataset(X_train, y_train)\n",
    "        val_dataset = ReviewDataset(X_val, y_val)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "        \n",
    "        # Initialize model\n",
    "        model = RatingPredictor(self.bert_dim, self.num_categorical).to(self.device)\n",
    "        \n",
    "        # Loss and optimizer\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=2e-4, weight_decay=0.01)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=2, verbose=True\n",
    "        )\n",
    "        \n",
    "        # Training loop\n",
    "        best_val_loss = float('inf')\n",
    "        best_model = None\n",
    "        no_improve = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "            \n",
    "            # Train\n",
    "            train_loss = self.train_epoch(model, train_loader, criterion, optimizer)\n",
    "            \n",
    "            # Validate\n",
    "            model.eval()\n",
    "            val_predictions = []\n",
    "            val_actuals = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for X_batch, y_batch in val_loader:\n",
    "                    X_batch = X_batch.to(self.device)\n",
    "                    y_pred = model(X_batch)\n",
    "                    val_predictions.extend(y_pred.cpu().numpy())\n",
    "                    val_actuals.extend(y_batch.numpy())\n",
    "            \n",
    "            val_predictions = np.array(val_predictions)\n",
    "            val_actuals = np.array(val_actuals)\n",
    "            \n",
    "            val_mse = mean_squared_error(val_actuals, val_predictions)\n",
    "            val_r2 = r2_score(val_actuals, val_predictions)\n",
    "            \n",
    "            print(f\"Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"Val MSE: {val_mse:.4f}\")\n",
    "            print(f\"Val R2: {val_r2:.4f}\")\n",
    "            \n",
    "            # Learning rate scheduling\n",
    "            scheduler.step(val_mse)\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_mse < best_val_loss:\n",
    "                best_val_loss = val_mse\n",
    "                best_model = model.state_dict()\n",
    "                no_improve = 0\n",
    "            else:\n",
    "                no_improve += 1\n",
    "                if no_improve >= patience:\n",
    "                    print(\"Early stopping triggered\")\n",
    "                    break\n",
    "        \n",
    "        # Load best model\n",
    "        model.load_state_dict(best_model)\n",
    "        return model\n",
    "\n",
    "    def make_predictions(self, model, X_test):\n",
    "        model.eval()\n",
    "        test_dataset = ReviewDataset(X_test)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "        predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch in test_loader:\n",
    "                X_batch = X_batch.to(self.device)\n",
    "                y_pred = model(X_batch)\n",
    "                predictions.extend(y_pred.cpu().numpy())\n",
    "        \n",
    "        return np.clip(np.array(predictions), 1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9385685-adc5-44d4-a964-fb22638d92f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7969/7969 [01:27<00:00, 90.76it/s, loss=0.7600]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6038\n",
      "Val MSE: 0.5361\n",
      "Val R2: 0.5644\n",
      "\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7969/7969 [01:28<00:00, 90.08it/s, loss=0.6812]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5507\n",
      "Val MSE: 0.5266\n",
      "Val R2: 0.5722\n",
      "\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7969/7969 [01:27<00:00, 90.87it/s, loss=0.5180]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5391\n",
      "Val MSE: 0.5273\n",
      "Val R2: 0.5715\n",
      "\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7969/7969 [01:28<00:00, 90.37it/s, loss=0.6258]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5316\n",
      "Val MSE: 0.5189\n",
      "Val R2: 0.5784\n",
      "\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7969/7969 [01:28<00:00, 89.77it/s, loss=0.4356]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5262\n",
      "Val MSE: 0.5117\n",
      "Val R2: 0.5843\n",
      "\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7969/7969 [01:28<00:00, 89.82it/s, loss=0.4488]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5211\n",
      "Val MSE: 0.5094\n",
      "Val R2: 0.5861\n",
      "\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7969/7969 [01:29<00:00, 89.21it/s, loss=0.4055]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5170\n",
      "Val MSE: 0.5129\n",
      "Val R2: 0.5832\n",
      "\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7969/7969 [01:28<00:00, 89.67it/s, loss=0.4444]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5133\n",
      "Val MSE: 0.5124\n",
      "Val R2: 0.5837\n",
      "\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7969/7969 [01:29<00:00, 89.45it/s, loss=0.5961]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5094\n",
      "Val MSE: 0.5107\n",
      "Val R2: 0.5851\n",
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-04.\n",
      "\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7969/7969 [01:29<00:00, 89.39it/s, loss=0.4213]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4976\n",
      "Val MSE: 0.5054\n",
      "Val R2: 0.5894\n",
      "\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7969/7969 [01:29<00:00, 89.13it/s, loss=0.5206]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4932\n",
      "Val MSE: 0.5052\n",
      "Val R2: 0.5896\n",
      "\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7969/7969 [01:28<00:00, 90.25it/s, loss=0.4381]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4900\n",
      "Val MSE: 0.5039\n",
      "Val R2: 0.5906\n",
      "\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7969/7969 [01:28<00:00, 90.04it/s, loss=0.6143]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4870\n",
      "Val MSE: 0.5040\n",
      "Val R2: 0.5905\n",
      "\n",
      "Epoch 14/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7969/7969 [01:28<00:00, 90.23it/s, loss=0.7461]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4841\n",
      "Val MSE: 0.5084\n",
      "Val R2: 0.5869\n",
      "\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7969/7969 [01:28<00:00, 89.72it/s, loss=0.4926]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4807\n",
      "Val MSE: 0.5047\n",
      "Val R2: 0.5899\n",
      "Epoch 00015: reducing learning rate of group 0 to 5.0000e-05.\n",
      "\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7969/7969 [01:30<00:00, 87.92it/s, loss=0.3737]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4719\n",
      "Val MSE: 0.5047\n",
      "Val R2: 0.5900\n",
      "\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7969/7969 [01:28<00:00, 89.65it/s, loss=0.3685]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4687\n",
      "Val MSE: 0.5052\n",
      "Val R2: 0.5895\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "# Calculate dimensions\n",
    "bert_dim = len([col for col in X_train.columns if col.startswith('emb_')])\n",
    "num_categorical = len([col for col in X_train.columns if not col.startswith('emb_')])\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = DeepLearningTrainer(bert_dim//3, num_categorical)  # divide by 3 because we have 3 text inputs\n",
    "\n",
    "# Train model\n",
    "model = trainer.train_and_evaluate(X_train, X_val, y_train, y_val)\n",
    "\n",
    "# Make predictions\n",
    "predictions = trainer.make_predictions(model, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5bf403-8dea-4645-88b5-49b71598f115",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_neural_net_submission(trainer, model, X_test, X_train, y_train, student_id=\"20886030\", anonymous_name=\"zesty\"):\n",
    "    \"\"\"\n",
    "    Create submission file for neural network model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    trainer : DeepLearningTrainer\n",
    "        The trainer object used to make predictions\n",
    "    model : RatingPredictor\n",
    "        The trained neural network model\n",
    "    X_test : pd.DataFrame\n",
    "        Test data\n",
    "    X_train : pd.DataFrame\n",
    "        Training data\n",
    "    y_train : pd.Series\n",
    "        Training labels\n",
    "    \"\"\"\n",
    "    # Get predictions using the trainer\n",
    "    predictions = trainer.make_predictions(model, X_test)\n",
    "    \n",
    "    # Calculate R² and MSE on training data using the trainer\n",
    "    train_predictions = trainer.make_predictions(model, X_train)\n",
    "    train_r2 = r2_score(y_train, train_predictions)\n",
    "    train_mse = mean_squared_error(y_train, train_predictions)\n",
    "    \n",
    "    # Model description\n",
    "    model_description = \"Deep Learning (DistilBERT Embeddings + Neural Network)\"\n",
    "    \n",
    "    # Create submission file\n",
    "    with open('submission_neural_net.csv', 'w') as f:\n",
    "        f.write(f\"{student_id}\\n\")\n",
    "        f.write(f\"{anonymous_name}\\n\")\n",
    "        f.write(f\"{train_r2:.4f}\\n\")\n",
    "        f.write(f\"{model_description}\\n\")\n",
    "        \n",
    "        # Write predictions\n",
    "        for pred in predictions:\n",
    "            f.write(f\"{pred:.4f}\\n\")\n",
    "    \n",
    "    print(f\"Submission file created with:\")\n",
    "    print(f\"Student ID: {student_id}\")\n",
    "    print(f\"Anonymous name: {anonymous_name}\")\n",
    "    print(f\"Training R²: {train_r2:.4f}\")\n",
    "    print(f\"Training MSE: {train_mse:.4f}\")  # Print the MSE\n",
    "    print(f\"Model: {model_description}\")\n",
    "    print(f\"Number of predictions: {len(predictions)}\")\n",
    "    \n",
    "    return 'submission_neural_net.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb218ce-14a8-4357-9426-567309ca932b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created with:\n",
      "Student ID: 20886030\n",
      "Anonymous name: zesty\n",
      "Training R²: 0.6388\n",
      "Training MSE: 0.4445\n",
      "Model: Deep Learning (DistilBERT Embeddings + Neural Network)\n",
      "Number of predictions: 100000\n"
     ]
    }
   ],
   "source": [
    "# After training the model:\n",
    "submission_file = create_neural_net_submission(\n",
    "    trainer=trainer,  # Pass the trainer object\n",
    "    model=model,\n",
    "    X_test=X_test,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad0f1c8-91f3-4bab-81e3-73d3650debdc",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27713b0c-18ba-48f0-abe0-b05df227718f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.9.4)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from wordcloud) (1.26.4)\n",
      "Requirement already satisfied: pillow in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from wordcloud) (11.0.0)\n",
      "Requirement already satisfied: matplotlib in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from wordcloud) (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib->wordcloud) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib->wordcloud) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib->wordcloud) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib->wordcloud) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib->wordcloud) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib->wordcloud) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib->wordcloud) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c468c4c2-8297-4be5-ab92-343546889a7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f29441-ab1d-48d0-b743-ec7ecf259c14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating visualizations...\n",
      "Creating word clouds...\n",
      "Analysis complete! Check 'word_frequencies.png' and 'word_clouds.png' for results.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "def load_original_data(small_train_path, large_train_path):\n",
    "    \"\"\"Load and combine the original training datasets\"\"\"\n",
    "    df_small = pd.read_csv(small_train_path)\n",
    "    df_large = pd.read_csv(large_train_path)\n",
    "    return pd.concat([df_small, df_large], axis=0, ignore_index=True)\n",
    "\n",
    "def analyze_word_frequencies(small_train_path, large_train_path):\n",
    "    # Load original data\n",
    "    print(\"Loading data...\")\n",
    "    df_train = load_original_data(small_train_path, large_train_path)\n",
    "    \n",
    "    # Download required NLTK data\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def clean_text(text):\n",
    "        \"\"\"Clean text by removing special characters and converting to lowercase\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        # Remove special characters and numbers\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
    "        # Remove extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        return text\n",
    "    \n",
    "    def get_word_freq(texts):\n",
    "        \"\"\"Get word frequencies excluding stopwords\"\"\"\n",
    "        words = []\n",
    "        for text in texts:\n",
    "            tokens = word_tokenize(clean_text(text))\n",
    "            words.extend([word for word in tokens if word not in stop_words and len(word) > 2])\n",
    "        return Counter(words)\n",
    "    \n",
    "    print(\"Creating visualizations...\")\n",
    "    # Create figure with subplots\n",
    "    plt.figure(figsize=(20, 12))\n",
    "    \n",
    "    # Process pros and cons separately for each rating\n",
    "    for idx, text_type in enumerate(['pros', 'cons']):\n",
    "        plt.subplot(1, 2, idx+1)\n",
    "        \n",
    "        # Store frequencies for each rating\n",
    "        frequencies = []\n",
    "        ratings = sorted(df_train['rating'].unique())\n",
    "        \n",
    "        for rating in ratings:\n",
    "            texts = df_train[df_train['rating'] == rating][text_type]\n",
    "            freq = get_word_freq(texts)\n",
    "            # Get top 10 words\n",
    "            top_words = dict(sorted(freq.items(), key=lambda x: x[1], reverse=True)[:10])\n",
    "            frequencies.append(pd.Series(top_words))\n",
    "        \n",
    "        # Create DataFrame with frequencies\n",
    "        freq_df = pd.DataFrame(frequencies, index=ratings)\n",
    "        \n",
    "        # Create heatmap\n",
    "        sns.heatmap(freq_df.fillna(0), \n",
    "                   cmap='YlOrRd', \n",
    "                   annot=True, \n",
    "                   fmt='.0f',\n",
    "                   cbar_kws={'label': 'Word Frequency'})\n",
    "        \n",
    "        plt.title(f'Top 10 Words in {text_type.capitalize()} by Rating')\n",
    "        plt.xlabel('Words')\n",
    "        plt.ylabel('Rating')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('word_frequencies.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    print(\"Creating word clouds...\")\n",
    "    # Create word clouds for each rating\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    for i, rating in enumerate(sorted(df_train['rating'].unique()), 1):\n",
    "        plt.subplot(3, 2, i)\n",
    "        \n",
    "        # Combine pros and cons for this rating\n",
    "        texts = (df_train[df_train['rating'] == rating]['pros'].fillna('') + ' ' + \n",
    "                df_train[df_train['rating'] == rating]['cons'].fillna(''))\n",
    "        \n",
    "        # Create and generate word cloud\n",
    "        text = ' '.join(texts)\n",
    "        wordcloud = WordCloud(width=800, height=400,\n",
    "                            background_color='white',\n",
    "                            max_words=50).generate(clean_text(text))\n",
    "        \n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.title(f'{rating}-Star Reviews')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('word_clouds.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Analysis complete! Check 'word_frequencies.png' and 'word_clouds.png' for results.\")\n",
    "\n",
    "# Example usage:\n",
    "analyze_word_frequencies(\n",
    "    small_train_path=\"424_F2024_Final_PC_small_train_v1.csv\",\n",
    "    large_train_path=\"424_F2024_Final_PC_large_train_v1.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f063e5-f881-4288-b7e9-52324785317b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_all_visualizations(trainer, model, X_train, X_test, y_train):\n",
    "    \"\"\"Create all visualizations using the trainer and model\"\"\"\n",
    "    \n",
    "    def create_feature_importance_plot(trainer, model, X_train, filename='feature_importance.png'):\n",
    "        print(\"Creating feature importance plot...\")\n",
    "        model.eval()\n",
    "        importances = []\n",
    "        \n",
    "        # Convert to tensor and move to same device as model\n",
    "        X = torch.FloatTensor(X_train.values).to(trainer.device)\n",
    "        \n",
    "        # Calculate importance for each feature\n",
    "        for i in range(X.shape[1]):\n",
    "            X_modified = X.clone()\n",
    "            X_modified[:, i] += torch.std(X[:, i]) * 0.1\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                original_output = model(X)\n",
    "                modified_output = model(X_modified)\n",
    "                importance = torch.mean(torch.abs(modified_output - original_output))\n",
    "                importances.append(importance.cpu().item())\n",
    "        \n",
    "        # Create importance DataFrame\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': X_train.columns,\n",
    "            'Importance': importances\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.barplot(x='Importance', y='Feature', data=importance_df.head(20))\n",
    "        plt.title('Top 20 Feature Importances')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(filename)\n",
    "        plt.close()\n",
    "        \n",
    "        return importance_df\n",
    "\n",
    "    # Alternative version if you want to include only the most important embedding features\n",
    "    def create_correlation_plot_with_top_embeddings(X_train, importance_df, top_n=10, filename='correlation_matrix.png'):\n",
    "        \"\"\"Create correlation matrix including only top important features\"\"\"\n",
    "        print(\"Creating correlation matrix plot with top features...\")\n",
    "\n",
    "        # Get non-embedding columns\n",
    "        non_embedding_cols = [col for col in X_train.columns if not col.startswith('emb_')]\n",
    "\n",
    "        # Get top important embedding features\n",
    "        top_embedding_cols = [col for col in importance_df['Feature'].head(top_n) \n",
    "                             if col.startswith('emb_')]\n",
    "\n",
    "        # Combine columns\n",
    "        selected_cols = non_embedding_cols + top_embedding_cols\n",
    "\n",
    "        # Calculate correlation matrix for selected features\n",
    "        corr_matrix = X_train[selected_cols].corr()\n",
    "\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(corr_matrix, \n",
    "                    cmap='RdBu_r', \n",
    "                    center=0,\n",
    "                    annot=True,\n",
    "                    fmt='.2f')\n",
    "        plt.title('Feature Correlation Matrix\\n(Basic Features + Top Important Embeddings)')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(filename)\n",
    "        plt.close()\n",
    "\n",
    "    def compare_distributions(X_train, X_test, filename='distribution_comparison.png'):\n",
    "        \"\"\"Compare feature distributions between train and test sets\"\"\"\n",
    "        print(\"Creating distribution comparison plots...\")\n",
    "        \n",
    "        # Select numerical features (limit to first 15 for visibility)\n",
    "        numerical_features = X_train.select_dtypes(include=['float64', 'int64']).columns[:15]\n",
    "        \n",
    "        n_features = len(numerical_features)\n",
    "        n_cols = 3\n",
    "        n_rows = (n_features + n_cols - 1) // n_cols\n",
    "        \n",
    "        plt.figure(figsize=(15, 5*n_rows))\n",
    "        \n",
    "        for idx, feature in enumerate(numerical_features, 1):\n",
    "            plt.subplot(n_rows, n_cols, idx)\n",
    "            \n",
    "            sns.kdeplot(data=X_train[feature], label='Train', alpha=0.5)\n",
    "            sns.kdeplot(data=X_test[feature], label='Test', alpha=0.5)\n",
    "            \n",
    "            plt.title(f'Distribution of {feature}')\n",
    "            plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(filename)\n",
    "        plt.close()\n",
    "\n",
    "    def analyze_prediction_errors(trainer, model, X_train, y_train, filename='prediction_errors.png'):\n",
    "        \"\"\"Analyze prediction errors across different rating levels\"\"\"\n",
    "        print(\"Creating prediction error analysis plots...\")\n",
    "        \n",
    "        # Get predictions using the trainer's prediction method\n",
    "        y_pred = trainer.make_predictions(model, X_train)\n",
    "        \n",
    "        # Create error analysis DataFrame\n",
    "        error_df = pd.DataFrame({\n",
    "            'Actual': y_train,\n",
    "            'Predicted': y_pred,\n",
    "            'Error': y_pred - y_train\n",
    "        })\n",
    "        \n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Plot 1: Actual vs Predicted\n",
    "        plt.subplot(2, 2, 1)\n",
    "        sns.scatterplot(data=error_df, x='Actual', y='Predicted', alpha=0.1)\n",
    "        plt.plot([1, 5], [1, 5], 'r--')\n",
    "        plt.title('Actual vs Predicted Ratings')\n",
    "        \n",
    "        # Plot 2: Error Distribution\n",
    "        plt.subplot(2, 2, 2)\n",
    "        sns.histplot(data=error_df, x='Error', bins=50)\n",
    "        plt.title('Distribution of Prediction Errors')\n",
    "        \n",
    "        # Plot 3: Box plot of errors by rating\n",
    "        plt.subplot(2, 2, 3)\n",
    "        sns.boxplot(data=error_df, x='Actual', y='Error')\n",
    "        plt.title('Prediction Errors by Actual Rating')\n",
    "        \n",
    "        # Plot 4: Mean Absolute Error by rating\n",
    "        plt.subplot(2, 2, 4)\n",
    "        mae_by_rating = error_df.groupby('Actual')['Error'].apply(lambda x: np.abs(x).mean())\n",
    "        sns.barplot(x=mae_by_rating.index, y=mae_by_rating.values)\n",
    "        plt.title('Mean Absolute Error by Rating')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(filename)\n",
    "        plt.close()\n",
    "    \n",
    "    # Create all visualizations\n",
    "    importance_df = create_feature_importance_plot(trainer, model, X_train)\n",
    "    create_correlation_plot_with_top_embeddings(X_train, importance_df)\n",
    "    compare_distributions(X_train, X_test)\n",
    "    analyze_prediction_errors(trainer, model, X_train, y_train)\n",
    "    \n",
    "    print(\"All visualizations have been created!\")\n",
    "    return importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eff7052-8f7d-4574-b87d-a602a19fa616",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating feature importance plot...\n",
      "Creating correlation matrix plot with top features...\n",
      "Creating distribution comparison plots...\n",
      "Creating prediction error analysis plots...\n",
      "All visualizations have been created!\n"
     ]
    }
   ],
   "source": [
    "visualizations = create_all_visualizations(\n",
    "    trainer=trainer,  # Your DeepLearningTrainer instance\n",
    "    model=model,      # Your trained model\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    y_train=y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876a2790-6954-4902-af10-4beb5dd53186",
   "metadata": {},
   "source": [
    "# Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3ce74139-6ede-43f3-987a-f7f83d94d896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def analyze_company_ratings(df_small_path, df_large_path):\n",
    "    \"\"\"\n",
    "    Analyze what drives ratings at different company quality levels.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_small_path : str\n",
    "        Path to small training dataset CSV\n",
    "    df_large_path : str\n",
    "        Path to large training dataset CSV\n",
    "    \"\"\"\n",
    "    # Download required NLTK data\n",
    "    try:\n",
    "        nltk.download('punkt')\n",
    "        nltk.download('stopwords')\n",
    "        nltk.download('averaged_perceptron_tagger')\n",
    "    except:\n",
    "        print(\"NLTK data already downloaded or error in downloading\")\n",
    "    \n",
    "    def load_and_prepare_data():\n",
    "        \"\"\"Load and prepare the datasets\"\"\"\n",
    "        # Load data\n",
    "        df_small = pd.read_csv(df_small_path)\n",
    "        df_large = pd.read_csv(df_large_path)\n",
    "        df = pd.concat([df_small, df_large], ignore_index=True)\n",
    "        \n",
    "        # Calculate company average ratings\n",
    "        company_ratings = df.groupby('firm')['rating'].agg(['mean', 'count']).reset_index()\n",
    "        \n",
    "        # Only consider companies with at least 10 reviews\n",
    "        company_ratings = company_ratings[company_ratings['count'] >= 10]\n",
    "        \n",
    "        # Categorize companies\n",
    "        company_ratings['quality_category'] = pd.qcut(\n",
    "            company_ratings['mean'], \n",
    "            q=3, \n",
    "            labels=['Low', 'Medium', 'High']\n",
    "        )\n",
    "        \n",
    "        # Create mapping dictionary\n",
    "        company_category_map = dict(zip(company_ratings['firm'], company_ratings['quality_category']))\n",
    "        \n",
    "        # Add categories to original dataframe\n",
    "        df['company_quality'] = df['firm'].map(company_category_map)\n",
    "        \n",
    "        return df, company_ratings\n",
    "    \n",
    "    def clean_text(text):\n",
    "        \"\"\"Clean text data\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to lowercase and remove special characters\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text.lower())\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def extract_key_phrases(texts, top_n=10):\n",
    "        \"\"\"Extract key phrases from text\"\"\"\n",
    "        # Combine all texts\n",
    "        combined_text = ' '.join([clean_text(text) for text in texts if isinstance(text, str)])\n",
    "        \n",
    "        # Tokenize\n",
    "        words = word_tokenize(combined_text)\n",
    "        \n",
    "        # Remove stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = [word for word in words if word not in stop_words and len(word) > 2]\n",
    "        \n",
    "        # Count frequencies\n",
    "        word_freq = Counter(words)\n",
    "        \n",
    "        return dict(word_freq.most_common(top_n))\n",
    "    \n",
    "    def analyze_topics_by_category(df):\n",
    "        \"\"\"Analyze topics for each company quality category\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for category in ['Low', 'Medium', 'High']:\n",
    "            category_df = df[df['company_quality'] == category]\n",
    "            \n",
    "            results[category] = {\n",
    "                'pros_topics': extract_key_phrases(category_df['pros']),\n",
    "                'cons_topics': extract_key_phrases(category_df['cons']),\n",
    "                'avg_rating': category_df['rating'].mean(),\n",
    "                'n_companies': category_df['firm'].nunique(),\n",
    "                'n_reviews': len(category_df)\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def create_visualizations(results):\n",
    "        \"\"\"Create visualizations for the analysis\"\"\"\n",
    "        # Set up the plots\n",
    "        fig, axes = plt.subplots(2, 1, figsize=(15, 20))\n",
    "        \n",
    "        # Plot pros\n",
    "        pros_data = []\n",
    "        for category in ['Low', 'Medium', 'High']:\n",
    "            for word, freq in results[category]['pros_topics'].items():\n",
    "                pros_data.append({\n",
    "                    'Category': category,\n",
    "                    'Word': word,\n",
    "                    'Frequency': freq\n",
    "                })\n",
    "        \n",
    "        pros_df = pd.DataFrame(pros_data)\n",
    "        sns.barplot(\n",
    "            data=pros_df,\n",
    "            x='Frequency',\n",
    "            y='Word',\n",
    "            hue='Category',\n",
    "            ax=axes[0]\n",
    "        )\n",
    "        axes[0].set_title('Most Common Positive Aspects by Company Quality')\n",
    "        \n",
    "        # Plot cons\n",
    "        cons_data = []\n",
    "        for category in ['Low', 'Medium', 'High']:\n",
    "            for word, freq in results[category]['cons_topics'].items():\n",
    "                cons_data.append({\n",
    "                    'Category': category,\n",
    "                    'Word': word,\n",
    "                    'Frequency': freq\n",
    "                })\n",
    "        \n",
    "        cons_df = pd.DataFrame(cons_data)\n",
    "        sns.barplot(\n",
    "            data=cons_df,\n",
    "            x='Frequency',\n",
    "            y='Word',\n",
    "            hue='Category',\n",
    "            ax=axes[1]\n",
    "        )\n",
    "        axes[1].set_title('Most Common Complaints by Company Quality')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('company_analysis.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Create summary table\n",
    "        summary_data = []\n",
    "        for category in ['Low', 'Medium', 'High']:\n",
    "            summary_data.append({\n",
    "                'Category': category,\n",
    "                'Average Rating': f\"{results[category]['avg_rating']:.2f}\",\n",
    "                'Number of Companies': results[category]['n_companies'],\n",
    "                'Number of Reviews': results[category]['n_reviews'],\n",
    "                'Top Pros': ', '.join(list(results[category]['pros_topics'].keys())[:5]),\n",
    "                'Top Cons': ', '.join(list(results[category]['cons_topics'].keys())[:5])\n",
    "            })\n",
    "        \n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        summary_df.to_csv('company_analysis_summary.csv', index=False)\n",
    "        \n",
    "        return summary_df\n",
    "    \n",
    "    # Main execution\n",
    "    print(\"Loading and preparing data...\")\n",
    "    df, company_ratings = load_and_prepare_data()\n",
    "    \n",
    "    print(\"Analyzing topics by category...\")\n",
    "    results = analyze_topics_by_category(df)\n",
    "    \n",
    "    print(\"Creating visualizations...\")\n",
    "    summary_df = create_visualizations(results)\n",
    "    \n",
    "    print(\"\\nAnalysis Complete!\")\n",
    "    print(\"\\nKey Findings:\")\n",
    "    print(\"-------------\")\n",
    "    for category in ['Low', 'Medium', 'High']:\n",
    "        print(f\"\\n{category}-rated companies:\")\n",
    "        print(f\"Average rating: {results[category]['avg_rating']:.2f}\")\n",
    "        print(f\"Number of companies: {results[category]['n_companies']}\")\n",
    "        print(\"Top pros:\", ', '.join(list(results[category]['pros_topics'].keys())[:5]))\n",
    "        print(\"Top cons:\", ', '.join(list(results[category]['cons_topics'].keys())[:5]))\n",
    "    \n",
    "    return results, summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db9b9faf-fbac-4fea-a312-62f86a285036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing topics by category...\n",
      "Creating visualizations...\n",
      "\n",
      "Analysis Complete!\n",
      "\n",
      "Key Findings:\n",
      "-------------\n",
      "\n",
      "Low-rated companies:\n",
      "Average rating: 3.39\n",
      "Number of companies: 138\n",
      "Top pros: good, work, great, people, company\n",
      "Top cons: work, management, pay, hours, get\n",
      "\n",
      "Medium-rated companies:\n",
      "Average rating: 3.72\n",
      "Number of companies: 137\n",
      "Top pros: good, work, great, people, company\n",
      "Top cons: work, hours, management, long, life\n",
      "\n",
      "High-rated companies:\n",
      "Average rating: 4.14\n",
      "Number of companies: 138\n",
      "Top pros: great, work, good, people, benefits\n",
      "Top cons: work, company, management, hours, get\n"
     ]
    }
   ],
   "source": [
    "results, summary = analyze_company_ratings(\n",
    "    \"424_F2024_Final_PC_small_train_v1.csv\",\n",
    "    \"424_F2024_Final_PC_large_train_v1.csv\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
